{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Module Import\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Classification.config as cfg\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "import spacy\n",
    "import pickle\n",
    "import time\n",
    "import operator\n",
    "import gensim\n",
    "import math\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.sparse import csc_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from ast import literal_eval\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob as tb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from Classification.germalemma import GermaLemma\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Importing the dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_csv(\"C:\\\\Users\\\\Josef\\\\PycharmProjects\\\\QC-Yes-No\\\\Classification\\\\preprocessing\\\\data\\\\data_ready.csv\", sep=';').drop('Unnamed: 0', 1)\n",
    "data_set['Feature'] = data_set['Feature'].apply(lambda x: x.split('#'))\n",
    "data_set['PosTags'] = data_set['PosTags'].apply(lambda x: x.split('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw.csv', 'w', encoding='UTF-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for _,line in data_set.iterrows():\n",
    "        a = ' '.join(line['Feature'][:-1])\n",
    "        writer.writerow([a, line['Label']])\n",
    "    f.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Defining the Classifier functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'algorithm': 'auto'\n",
    " - 'leaf_size': 30\n",
    " - 'metric': 'minkowski'\n",
    " - 'metric_params': None\n",
    " - 'n_jobs': 1\n",
    " - 'n_neighbors': 5\n",
    " - 'p': 2\n",
    " - 'weights': 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    kn_clf = KNeighborsClassifier(**parameters)\n",
    "    kn_clf.fit(X_train, y_train)\n",
    "    kn_accuracy = kn_clf.score(X_test, y_test)\n",
    "    return kn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'alpha': 1.0\n",
    " - 'class_prior': None\n",
    " - 'fit_prior': True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    nb_clf = MultinomialNB(**parameters)\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_accuracy = nb_clf.score(X_test, y_test)\n",
    "    return nb_accuracy\n",
    "\n",
    "def naive_bayes_C(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    nb_clf = ComplementNB(**parameters)\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_accuracy = nb_clf.score(X_test, y_test)\n",
    "    return nb_accuracy\n",
    "\n",
    "def naive_bayes_B(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    nb_clf = BernoulliNB(**parameters)\n",
    "    nb_clf.fit(X_train, y_train)\n",
    "    nb_accuracy = nb_clf.score(X_test, y_test)\n",
    "    return nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'class_weight': None\n",
    " - 'criterion': 'gini'\n",
    " - 'max_depth': None\n",
    " - 'max_features': None\n",
    " - 'max_leaf_nodes': None\n",
    " - 'min_impurity_decrease': 0.0\n",
    " - 'min_impurity_split': None\n",
    " - 'min_samples_leaf': 1\n",
    " - 'min_samples_split': 2\n",
    " - 'min_weight_fraction_leaf': 0.0\n",
    " - 'presort': False\n",
    " - 'random_state': None,\n",
    " - 'splitter': 'best'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "featureparams = {'fs_words': {}, \n",
    "               'fs_ngrams': {}, \n",
    "               'fs_pos': {'max_depth': 8, 'min_samples_leaf': 2}, \n",
    "               'fs_words_min': {'min_samples_leaf': 3}, \n",
    "               'fs_bigrams_min': {}, \n",
    "               'fs_tfidf_words': {}, \n",
    "               'fs_words_ngrams': {}, \n",
    "               'fs_words_pos': {}, \n",
    "               'fs_min_pos': {'min_samples_leaf': 3}, \n",
    "               'fs_ngrams_pos': {}, \n",
    "               'fs_words_ngrams_pos': {}, \n",
    "               'fs_w2v': {'max_depth': 8}, \n",
    "               'fs_d2v': {'max_depth': 5, 'min_samples_leaf': 3}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    dt_clf = DecisionTreeClassifier(**parameters)\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    dt_accuracy = dt_clf.score(X_test, y_test)\n",
    "    return dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'fs_words': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 20}, \n",
    "               'fs_ngrams': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_pos': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_words_min': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 20}, \n",
    "               'fs_bigrams_min': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 10}, \n",
    "               'fs_tfidf_words': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_words_ngrams': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 20}, \n",
    "               'fs_words_pos': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 20}, \n",
    "               'fs_min_pos': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_ngrams_pos': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_words_ngrams_pos': {'random_state': 1, 'n_estimators': 500}, \n",
    "               'fs_w2v': {'random_state': 1, 'n_estimators': 500, 'min_samples_split': 10}, \n",
    "               'fs_d2v': {'random_state': 1, 'n_estimators': 500}\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'bootstrap': True \n",
    " - 'class_weight': None \n",
    " - 'criterion': 'gini' \n",
    " - 'max_depth': None \n",
    " - 'max_features': 'auto'\n",
    " - 'max_leaf_nodes': None\n",
    " - 'min_impurity_decrease': 0.0\n",
    " - 'min_impurity_split': None\n",
    " - 'min_samples_leaf': 1\n",
    " - 'min_samples_split': 2\n",
    " - 'min_weight_fraction_leaf': 0.0\n",
    " - 'n_estimators': 10 \n",
    " - 'n_jobs': 1\n",
    " - 'oob_score': False\n",
    " - 'random_state': None\n",
    " - 'verbose': 0\n",
    " - 'warm_start': False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    rf_clf = RandomForestClassifier(**parameters)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    rf_accuracy = rf_clf.score(X_test, y_test)\n",
    "    return rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'C': 1.0\n",
    " - 'cache_size': 200\n",
    " - 'class_weight': None\n",
    " - 'coef0': 0.0\n",
    " - 'decision_function_shape': 'ovr'\n",
    " - 'degree': 3\n",
    " - 'gamma': 'auto'\n",
    " - 'kernel': 'rbf'\n",
    " - 'max_iter': -1\n",
    " - 'probability': False\n",
    " - 'random_state': None\n",
    " - 'shrinking': True\n",
    " - 'tol': 0.001\n",
    " - 'verbose': False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'fs_words': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_ngrams': {'random_state': 1, 'C': 0.75}, \n",
    "               'fs_pos': {'random_state': 1, 'C': 1}, \n",
    "               'fs_words_min': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_bigrams_min': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_tfidf_words': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_words_ngrams': {'random_state': 1, 'C': 0.75}, \n",
    "               'fs_words_pos': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_min_pos': {'random_state': 1, 'C': 0.5}, \n",
    "               'fs_ngrams_pos': {'random_state': 1, 'C': 2}, \n",
    "               'fs_words_ngrams_pos': {'random_state': 1, 'C': 1}, \n",
    "               'fs_w2v': {'random_state': 1, 'C': 2}, \n",
    "               'fs_d2v': {'random_state': 1, 'C': 2}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    svm_clf = SVC(**parameters)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_accuracy = svm_clf.score(X_test, y_test)\n",
    "    return svm_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'base_score': 0.5\n",
    " - 'booster': 'gbtree'\n",
    " - 'colsample_bylevel': 1\n",
    " - 'colsample_bytree': \n",
    " - 'gamma': 0\n",
    " - 'learning_rate': 0.1\n",
    " - 'max_delta_step': 0\n",
    " - 'max_depth': 3, \n",
    " - 'min_child_weight': 1\n",
    " - 'missing': None\n",
    " - 'n_estimators': 100\n",
    " - 'nthread': 1\n",
    " - 'objective': 'binary:logistic'\n",
    " - 'reg_alpha': 0\n",
    " - 'reg_lambda': 1\n",
    " - 'scale_pos_weight': 1\n",
    " - 'seed': 0\n",
    " - 'silent': 1 \n",
    " - 'subsample': 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XG_Boost(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    xg_clf = XGBClassifier(**parameters)\n",
    "    xg_clf.fit(X_train, y_train)\n",
    "    xg_accuracy = xg_clf.score(X_test, y_test)\n",
    "    return xg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(50, 2), random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    mlp_clf = MLPClassifier(**parameters)\n",
    "    mlp_clf.fit(X_train, y_train)\n",
    "    mlp_accuracy = xg_clf.score(X_test, y_test)\n",
    "    return xg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual rule based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class manual_classifier(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "            \n",
    "    def get_manual_label(self, row):\n",
    "        if (row.loc[row.index[0], 'PosTags'][0].startswith('V') or 'oder' in row.loc[row.index[0], 'Feature'][-2:]) \\\n",
    "                    and not 'oder' in row.loc[row.index[0], 'Feature'][:-2] \\\n",
    "                    and not 'jemand' in row.loc[row.index[0], 'Feature'][:-2] \\\n",
    "                    and not 'wer' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'was' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'welche' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'welchem' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'wieso' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'wo' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'warum' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'wie' in row.loc[row.index[0], 'Feature'] \\\n",
    "                    and not 'wann' in row.loc[row.index[0], 'Feature']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0 \n",
    "    \n",
    "    def classify(self, data):\n",
    "        label = []\n",
    "        for i in data.index.values.tolist():\n",
    "            label.append(self.get_manual_label(data.loc[[i]]))\n",
    "        return(label)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "        labels = self.classify(data)\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        total_pos = data['Label'].values.tolist().count(1)\n",
    "        found_pos = labels.count(1)\n",
    "        for index, value in enumerate(labels):\n",
    "            if value != data.loc[index, 'Label']:\n",
    "                if value == 1:\n",
    "                    false_positive += 1\n",
    "                else:\n",
    "                    false_negative += 1\n",
    "                    print(index, data.loc[index, 'Feature'], value, data.loc[index, 'Label'])\n",
    "\n",
    "        print('Accuracy:', (len(labels) - (false_positive + false_negative)) / len(labels))\n",
    "        print('False Positive: ', false_positive / len(labels))\n",
    "        print('False Negative: ', false_negative / len(labels))\n",
    "        print('Precision: ', (found_pos - false_positive) / found_pos)\n",
    "        print('Recall: ', (found_pos - false_positive) / total_pos)\n",
    "        return((len(labels) - (false_positive + false_negative)) / len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(featureset, runs=5, test_size=0.2, f_col='Feature'):\n",
    "    algs =  {'k_nearest' : {'n_jobs' : -1}, \n",
    "             'naive_bayes': {}, \n",
    "             'random_forest': {'n_estimators': 100, 'n_jobs': -1, 'min_samples_split': 10},\n",
    "             'decision_tree': {},\n",
    "             'SVM': {'kernel' : 'linear'}, \n",
    "             'XG_Boost' : {'max_depth' : 5, 'n_estimators' : 125, 'learning_rate' : 0.1, 'min_child_weight' : 1, 'njobs' : -1}, \n",
    "             'MLP': {'solver': 'lbfgs', alpha: 1e-5, hidden_layer_sizes: (50, 2), random_state: 1}}\n",
    "\n",
    "    for alg in algs:\n",
    "        get_average_accuracy(alg, runs, featureset, test_size, algs[alg], f_col=f_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining the train-test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(feature_set, test_size=0.2, f_col='Feature', l_col='Label', random_state=None):\n",
    "    X, y = pd.DataFrame(feature_set[f_col]), pd.DataFrame(feature_set[l_col])\n",
    "    X = np.array(list(X[f_col]))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Defining the average accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_accuracy(function_name, iteration_amount, data_set, test_size, parameters, f_col='Feature', l_col='Label'):\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(iteration_amount):      \n",
    "        X_train, X_test, y_train, y_test = get_train_test(data_set, test_size, f_col, l_col)\n",
    "        results.append(globals()[function_name](X_train, X_test, y_train, y_test, parameters))\n",
    "        \n",
    "        print(\"'Evaluated {0} questions with {1}. Average accuracy: {2}. Time taken: {3}\".format(i + 1, function_name, sum(results) / (i + 1), time.time() - start_time), end='\\r')\n",
    "    print('\\n')\n",
    "    return sum(results) / iteration_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Defining the problematic question evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_location(index):\n",
    "        found = data_set_index.loc[(data_set_index['Start'] <= index) & (data_set_index['End'] >= index)]\n",
    "        found_info = found['File Name'].item().split('\\\\Corpus\\\\')[1]\n",
    "        found_type, found_file = found_info.split('\\\\')[0], found_info.split('\\\\')[2]\n",
    "        index_in_file = index - found['Start'].item() + 1\n",
    "        return found_type, found_file, index_in_file\n",
    "    \n",
    "def evaluate(test_frame, predictions):\n",
    "    with open('checked_list.pickle', 'rb') as f:\n",
    "        checked_rows = pickle.load(f)\n",
    "    \n",
    "    test_frame_local = deepcopy(test_frame)\n",
    "    test_frame_local.columns = ['Label']\n",
    "    test_frame_local['Prediction'] = preds\n",
    "    false_rows = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in test_frame_local.iterrows():\n",
    "        i=1\n",
    "        if row['Prediction'] != row['Label']:\n",
    "            i+=1\n",
    "            loc = find_dataset_location(index)\n",
    "            false_rows.append([index, data_set[index][0], row['Label'], row['Prediction'], loc[0], loc[1], loc[2]])\n",
    "            \n",
    "    false_rows.sort(key=operator.itemgetter(0))\n",
    "    evaluation = pd.DataFrame(false_rows, columns=['Idx', 'Question', 'Label', 'Pred', 'Type', 'File', 'Line in File'])\n",
    "    display('Found {0} possibly wrongly labeled questions.'.format(len(evaluation.loc[~evaluation['Idx'].isin(checked_rows)])))\n",
    "    display(evaluation.loc[~evaluation['Idx'].isin(checked_rows)])\n",
    "    checked_rows.update(evaluation['Idx'].values)\n",
    "\n",
    "    with open('checked_list.pickle', 'wb') as f:\n",
    "        pickle.dump(checked_rows, f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle(data, name):\n",
    "    with open('pickles/' + name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=2)\n",
    "\n",
    "def load_pickle(name):\n",
    "    with open('pickles/' + name + '.pickle', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Creating words based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of words. Amount of words: 5717\n"
     ]
    }
   ],
   "source": [
    "bag_words = set(word for row in data_set['Feature'] for word in row)\n",
    "print('Created bag of words. Amount of words: {0}'.format(len(bag_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of words. Amount of words: 5717\n",
      "Encoded bag of words feature set.\n",
      "Time taken: 311.83122873306274\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fs_words = pd.DataFrame([([(word in data['Feature']) for word in bag_words], data['Label']) for _, data in data_set.iterrows()], columns=['Feature', 'Label'])\n",
    "print('Encoded bag of words feature set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickle(fs_words, 'fs_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words = load_pickle('fs_words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Getting data points for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_word_train, X_word_test, y_word_train, y_word_test = get_train_test(fs_words, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bag of ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 Creating bigrams of words based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of ngrams. Amount of ngrams: 18922\n"
     ]
    }
   ],
   "source": [
    "bag_ngrams = set(gram for row in data_set['Feature'] for gram in ngrams(row, 2))\n",
    "print('Created bag of ngrams. Amount of ngrams: {0}'.format(len(bag_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of ngrams. Amount of ngrams: 18922\n",
      "Encoded bag of ngrams feature set.\n",
      "Time taken: 1407.278207540512\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fs_ngrams= pd.DataFrame([([gram in ngrams(data['Feature'], 2) for gram in bag_ngrams], data['Label']) for _, data in data_set.iterrows()], columns=['Feature', 'Label'])\n",
    "print('Encoded bag of ngrams feature set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickle(fs_ngrams, 'fs_ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ngrams = load_pickle('fs_ngrams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Getting data points for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0019922256469726562\n"
     ]
    }
   ],
   "source": [
    "X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test = get_train_test(fs_ngrams, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Part-of-Speech Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Creating postags based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of PosTags. Amount of tags: 47\n"
     ]
    }
   ],
   "source": [
    "bag_tags = set(tag for row in data_set['PosTags'] for tag in row)\n",
    "bag_tags_bigrams = set(tag for gram  in ngrams(data_set['PosTags'], 2) for tag in row)\n",
    "bag_tags_trigrams = set(tag for gram  in ngrams(data_set['PosTags'], 3) for tag in row)\n",
    "print('Created bag of PosTags. Amount of tags: {0}'.format(len(bag_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of PosTags. Amount of tags: 47\n",
      "Encoded bag of PosTags feature set.\n",
      "Time taken: 2.9926722049713135\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fs_pos = pd.DataFrame([([(tag in row['PosTags']) for tag in bag_tags], row['Label']) for _, row in data_set.iterrows()], columns=['Feature', 'Label'])\n",
    "fs_pos_bigram = pd.DataFrame([([(tag in ngrams(row['PosTags'], 2)) for tag in bag_tags_bigrams], row['Label']) for _, row in data_set.iterrows()], columns=['Feature', 'Label'])\n",
    "fs_pos_trigrams = pd.DataFrame([([(tag in ngrams(row['PosTags'], 3)) for tag in bag_tags_trigrams], row['Label']) for _, row in data_set.iterrows()], columns=['Feature', 'Label'])\n",
    "print('Encoded bag of PosTags feature set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pickle(fs_pos, 'fs_pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_pos = load_pickle('fs_pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Getting data points for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_train, X_pos_test, y_pos_train, y_pos_test = get_train_test(fs_pos, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 8 Combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Creating feature count for words with minimum count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a minimum frequency for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "min_word_count = 3\n",
    "\n",
    "\n",
    "sents = data_set['Feature'].values.tolist()\n",
    "sent_merged = [j for i in sents for j in i]\n",
    "word_count = Counter(sent_merged)\n",
    "\n",
    "bag_words_min = set(word for word in bag_words if word_count[word] >= min_word_count)\n",
    "\n",
    "min_word_frame = deepcopy(data_set)\n",
    "min_word_frame['Feature_min'] = ''\n",
    "\n",
    "for index, row in min_word_frame.iterrows():\n",
    "    f = []\n",
    "    for word in row['Feature']:\n",
    "        if word_count[word] >= min_word_count:\n",
    "            f.append(word)\n",
    "    min_word_frame.at[index, 'Feature_min'] = f\n",
    "    \n",
    "fs_words_min = pd.DataFrame([([(word in data['Feature_min']) for word in bag_words_min], data['Label']) for _, data in min_word_frame.iterrows()], columns=['Feature', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 5 questions with k_nearest. Average accuracy: 0.8163509471585245. Time taken: 10.457966804504395\n",
      "\n",
      "'Evaluated 5 questions with naive_bayes. Average accuracy: 0.8743768693918245. Time taken: 0.62313437461853034\n",
      "\n",
      "'Evaluated 5 questions with random_forest. Average accuracy: 0.9373878364905284. Time taken: 5.6189074516296394\n",
      "\n",
      "'Evaluated 5 questions with decision_tree. Average accuracy: 0.9347956131605184. Time taken: 4.7906150817871092\n",
      "\n",
      "'Evaluated 5 questions with SVM. Average accuracy: 0.9457627118644067. Time taken: 32.224030256271368\n",
      "\n",
      "'Evaluated 5 questions with XG_Boost. Average accuracy: 0.9443668993020937. Time taken: 187.94302034378052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_all(fs_words_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a minimum frequency for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count = {}\n",
    "min_bigram_count = 2\n",
    "bigrams = [j for i in sents for j in ngrams(i, 2)]\n",
    "bigram_count = Counter(bigrams)\n",
    "\n",
    "#bag_ngrams_min = set(bigram for bigram in bag_ngrams if bigram_count[(bigram[0], bigram[1])] >= min_bigram_count)\n",
    "\n",
    "\n",
    "\n",
    "bag_ngrams_min = set(gram for row in min_word_frame['Feature_min'] for gram in ngrams(row, 2))\n",
    "\n",
    "min_bigram_frame = deepcopy(data_set)\n",
    "min_bigram_frame['Feature_min'] = ''\n",
    "\n",
    "for index, row in min_bigram_frame.iterrows():\n",
    "    f = []\n",
    "    for i, bigram in enumerate(ngrams(row['Feature'], 2)):\n",
    "        if bigram_count[(bigram[0], bigram[1])] >= min_bigram_count:\n",
    "            f.append(bigram)\n",
    "    min_bigram_frame.at[index, 'Feature_min'] = f\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fs_bigrams_min = pd.DataFrame([([gram in data['Feature_min'] for gram in bag_ngrams_min], data['Label']) for _, data in min_bigram_frame.iterrows()], columns=['Feature', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(n_clusters=2)\n",
    "clf.fit(X_word_train)\n",
    "pred = clf.predict(X_word_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5174476570289133\n",
      "484\n"
     ]
    }
   ],
   "source": [
    "wrong = 0\n",
    "for i, x in enumerate(pred):\n",
    "    if y_word_test['Label'].values.tolist()[i] != x:\n",
    "        wrong += 1\n",
    "print((len(pred) - wrong) / len(pred))\n",
    "print(wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Creating combined features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words_pos = deepcopy(fs_pos)\n",
    "fs_words_pos['Feature'] = fs_words['Feature'] + fs_word_pos['Feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 2 questions with k_nearest. Average accuracy: 0.8773678963110667. Time taken: 23.500316619873047\n",
      "\n",
      "'Evaluated 2 questions with naive_bayes. Average accuracy: 0.9027916251246262. Time taken: 1.0892376899719238\n",
      "\n",
      "'Evaluated 2 questions with random_forest. Average accuracy: 0.9466600199401795. Time taken: 8.268470287322998\n",
      "\n",
      "'Evaluated 2 questions with decision_tree. Average accuracy: 0.9506480558325026. Time taken: 5.6662356853485116\n",
      "\n",
      "'Evaluated 2 questions with SVM. Average accuracy: 0.9586241276171485. Time taken: 49.330788135528564\n",
      "\n",
      "'Evaluated 2 questions with XG_Boost. Average accuracy: 0.9675972083748754. Time taken: 196.55791854858398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_all(fs_word_pos, runs=2,  f_col='Combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word min + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_min_pos = deepcopy(fs_pos)\n",
    "fs_min_pos['Feature'] = fs_words_min['Feature'] + fs_min_pos['Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words_ngrams = deepcopy(fs_ngrams)\n",
    "fs_ngrams_pos['Feature'] = fs_words['Feature'] + fs_ngrams['Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ngrams_pos = deepcopy(fs_ngrams)\n",
    "fs_ngrams_pos['Feature'] = fs_ngrams['Feature'] + fs_min_pos['Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + bigram + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words_ngrams_pos = deepcopy(fs_ngrams)\n",
    "fs_words_ngrams_pos['Feature'] = fs_words['Feature'] + fs_ngrams['Feature'] + fs_pos['Feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the featureset to TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_frame = pd.DataFrame(index=range(len(data_set)), columns=bag_words).fillna(0.0)\n",
    "data_set_sent = [' '.join(data['Feature']) for _, data in data_set.iterrows()]\n",
    "\n",
    "bloblist = [tb(i) for i in data_set_sent]\n",
    "tfidf_list = []\n",
    "for i, blob in enumerate(bloblist):\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    for word in scores:\n",
    "        tfidf_frame.at[i, word] = round(scores[word], 5)\n",
    "    tfidf_list.append(scores)\n",
    "\n",
    "tfidf_values = tfidf_frame.values.tolist()\n",
    "fs_tfidf_words = deepcopy(fs_words)\n",
    "fs_tfidf_words['Feature'] = tfidf_values\n",
    "data_set['TfIdf'] = tfidf_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec model.\n"
     ]
    }
   ],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('german.model', binary=True,)\n",
    "print('Loaded word2vec model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function that returns an array of vectors of a given line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v(word):\n",
    "    try:\n",
    "        vec = w2v_model.get_vector(word)\n",
    "        return np.round(vec, 8)\n",
    "    except:\n",
    "        vec = np.zeros(shape=(1, 300))\n",
    "        return vec[0]\n",
    "\n",
    "def get_vectors(text):\n",
    "    vectors = {}\n",
    "    for word in text[:-1]:\n",
    "            vec[i] = get_w2v(word)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the featureset using a weighted average (TfIdf) of the phrase vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_w2v = deepcopy(data_set)\n",
    "data_set_w2v['Word2Vec'] = ''\n",
    "\n",
    "bag_w2v = {}\n",
    "for word in bag_words:\n",
    "    bag_w2v[word] = get_w2v(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in data_set_w2v.iterrows():\n",
    "    avg_vecs = []\n",
    "    word_len = 0\n",
    "    for j, word in enumerate(row['Feature'][:-1]):\n",
    "        vec = bag_w2v[word]\n",
    "        if not sum(vec) == 0:\n",
    "            word_len += 1\n",
    "            \n",
    "        avg_vecs.append(list(vec * row['TfIdf'][word]))\n",
    "        \n",
    "    if word_len > 0:\n",
    "        data_set_w2v.at[i, 'Word2Vec'] = np.array(avg_vecs).sum(axis=0) / word_len\n",
    "    else: \n",
    "        data_set_w2v.at[i, 'Word2Vec'] = np.array(avg_vecs).sum(axis=0)\n",
    "        \n",
    "w2v_values = list(np.round(data_set_w2v['Word2Vec'].values.tolist(), 6))\n",
    "fs_w2v = deepcopy(fs_words)\n",
    "fs_w2v['Feature'] = w2v_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 10 questions with k_nearest. Average accuracy: 0.639581256231306. Time taken: 4.5955443382263185\n",
      "\n",
      "'Evaluated 10 questions with random_forest. Average accuracy: 0.6959122632103688. Time taken: 7.229034662246704\n",
      "\n",
      "'Evaluated 10 questions with decision_tree. Average accuracy: 0.5897308075772681. Time taken: 9.908966064453125\n",
      "\n",
      "'Evaluated 10 questions with SVM. Average accuracy: 0.702791625124626. Time taken: 40.528583526611332\n",
      "\n",
      "'Evaluated 10 questions with XG_Boost. Average accuracy: 0.7082751744765703. Time taken: 163.2045726776123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_all(fs_w2v, 'Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(x, tokens_only=False):\n",
    "    for i, line in x.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(line['Feature'])\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(line['Feature'], tags=[line['Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "shuffled = data_set.sample(frac=1)\n",
    "d_train, d_test = shuffled.head(int(len(shuffled) * 0.8)), shuffled.tail(len(shuffled) - int(len(shuffled) * 0.8))\n",
    "train_corpus = list(read_corpus(d_train))\n",
    "test_corpus = list(read_corpus(d_test))\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=2, epochs=40)\n",
    "model.build_vocab(train_corpus)\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9206586826347305\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for line, idx in train_corpus:\n",
    "    inferred_vector = model.infer_vector(line)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    pred_label = []\n",
    "    for i, value in sims[:1]:\n",
    "        pred_label.append(data_set.loc[i, 'Label'])\n",
    "    labels.append([int(sum(pred_label) / len(pred_label)), int(data_set.loc[idx, 'Label'])])\n",
    "\n",
    "wrong = 0\n",
    "total = len(labels)\n",
    "for x,y in labels:\n",
    "    if x != y:\n",
    "        wrong += 1\n",
    "print(1 - wrong/total)\n",
    "\n",
    "fs_d2v = deepcopy(data_set)\n",
    "for i, row in data_set_d2v.iterrows():\n",
    "    fs_d2v.at[i, 'Feature'] = model.infer_vector(row['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 10 questions with XG_Boost. Average accuracy: 0.8674975074775674. Time taken: 127.69767332077026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_all(data_set_d2v, f_col='Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 100 questions with naive_bayes. Average accuracy: 0.8621036889332. Time taken: 55.35055494308472866\n",
      "\n",
      "'Evaluated 100 questions with naive_bayes_x. Average accuracy: 0.8616849451645063. Time taken: 55.67763805389404\n",
      "\n",
      "'Evaluated 100 questions with naive_bayes_y. Average accuracy: 0.8686440677966101. Time taken: 62.08309197425842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8686440677966101"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = fs_words\n",
    "runs = 100\n",
    "get_average_accuracy('naive_bayes', runs, fs, 0.2, {})\n",
    "get_average_accuracy('naive_bayes_x', runs, fs, 0.2, {})\n",
    "get_average_accuracy('naive_bayes_y', runs, fs, 0.2, {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.grid_search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-258-bcfc641e39b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_search\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Choose all predictors except target & IDcols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m param_test1 =  {\n\u001b[0;32m      4\u001b[0m  \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  \u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.grid_search'"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "param_test1 =  {\n",
    " 'learning_rate':[0.125, 1.5],\n",
    " 'n_estimators':[75, 100],\n",
    " 'max_depth': [5, 6, 7],\n",
    " 'min_child_weight': [0.5, 1],\n",
    " 'subsample': [0.5, 1]}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.125,\n",
    "                                                      max_depth = 5,\n",
    "                                                      random_state= 10,\n",
    "                                                      min_child_weight = 1), \n",
    "param_grid = param_test1, n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.estimator.get_params()\n",
    "gsearch1.fit(np.array(x_word_min_train), np.array(y_word_min_train))\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(xg_class_word, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(30, 30))\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgb.plot_tree(xg_class_word, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Evaluated 5 questions with k_nearest. Average accuracy: 0.8167497507477567. Time taken: 59.298043251037686\n",
      "\n",
      "'Evaluated 5 questions with naive_bayes. Average accuracy: 0.8528414755732803. Time taken: 2.6805853843688965\n",
      "\n",
      "'Evaluated 5 questions with random_forest. Average accuracy: 0.9335992023928215. Time taken: 31.888021230697632\n",
      "\n",
      "'Evaluated 5 questions with decision_tree. Average accuracy: 0.9405782652043868. Time taken: 21.057191371917725\n",
      "\n",
      "'Evaluated 5 questions with SVM. Average accuracy: 0.9517447657028913. Time taken: 189.21160888671875\n",
      "\n",
      "'Evaluated 5 questions with XG_Boost. Average accuracy: 0.9397806580259223. Time taken: 467.11976194381714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_ngram_pos = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words = load_pickle('featuresets/fs_words')\n",
    "fs_ngrams = load_pickle('featuresets/fs_ngrams')\n",
    "fs_pos = load_pickle('featuresets/fs_pos') \n",
    "fs_words_min = load_pickle('featuresets/fs_words_min')\n",
    "fs_ngrams_min = load_pickle('featuresets/fs_ngrams_min')\n",
    "fs_tfidf_words = load_pickle('featuresets/fs_tfidf_words')\n",
    "fs_words_ngrams = load_pickle('featuresets/fs_words_ngrams')\n",
    "fs_words_pos = load_pickle('featuresets/fs_words_pos')\n",
    "fs_words_min_pos = load_pickle('featuresets/fs_min_pos')\n",
    "fs_ngrams_pos = load_pickle('featuresets/fs_ngrams_pos')\n",
    "fs_words_ngrams_pos = load_pickle('featuresets/fs_words_ngrams_pos')\n",
    "fs_w2v = load_pickle('featuresets/fs_w2v')\n",
    "fs_d2v = load_pickle('featuresets/fs_d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(fs_words, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(5717, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4008/4008 [==============================] - ETA: 3:40 - loss: 0.6901 - acc: 0.500 - ETA: 2:02 - loss: 0.6860 - acc: 0.531 - ETA: 1:29 - loss: 0.6765 - acc: 0.572 - ETA: 1:13 - loss: 0.6679 - acc: 0.625 - ETA: 1:03 - loss: 0.6573 - acc: 0.662 - ETA: 56s - loss: 0.6573 - acc: 0.656 - ETA: 52s - loss: 0.6557 - acc: 0.64 - ETA: 48s - loss: 0.6577 - acc: 0.63 - ETA: 45s - loss: 0.6569 - acc: 0.62 - ETA: 43s - loss: 0.6434 - acc: 0.64 - ETA: 41s - loss: 0.6354 - acc: 0.65 - ETA: 39s - loss: 0.6327 - acc: 0.66 - ETA: 37s - loss: 0.6242 - acc: 0.66 - ETA: 36s - loss: 0.6116 - acc: 0.68 - ETA: 35s - loss: 0.6074 - acc: 0.68 - ETA: 34s - loss: 0.5967 - acc: 0.69 - ETA: 33s - loss: 0.5855 - acc: 0.70 - ETA: 32s - loss: 0.5780 - acc: 0.71 - ETA: 31s - loss: 0.5724 - acc: 0.72 - ETA: 30s - loss: 0.5675 - acc: 0.72 - ETA: 30s - loss: 0.5590 - acc: 0.73 - ETA: 29s - loss: 0.5506 - acc: 0.73 - ETA: 28s - loss: 0.5431 - acc: 0.74 - ETA: 28s - loss: 0.5391 - acc: 0.74 - ETA: 27s - loss: 0.5371 - acc: 0.74 - ETA: 27s - loss: 0.5296 - acc: 0.75 - ETA: 26s - loss: 0.5243 - acc: 0.75 - ETA: 26s - loss: 0.5157 - acc: 0.76 - ETA: 25s - loss: 0.5143 - acc: 0.76 - ETA: 25s - loss: 0.5068 - acc: 0.76 - ETA: 25s - loss: 0.4992 - acc: 0.77 - ETA: 24s - loss: 0.4983 - acc: 0.77 - ETA: 24s - loss: 0.4933 - acc: 0.77 - ETA: 23s - loss: 0.4928 - acc: 0.77 - ETA: 23s - loss: 0.4904 - acc: 0.77 - ETA: 23s - loss: 0.4853 - acc: 0.78 - ETA: 22s - loss: 0.4793 - acc: 0.78 - ETA: 22s - loss: 0.4717 - acc: 0.78 - ETA: 21s - loss: 0.4679 - acc: 0.79 - ETA: 21s - loss: 0.4639 - acc: 0.79 - ETA: 21s - loss: 0.4640 - acc: 0.79 - ETA: 20s - loss: 0.4645 - acc: 0.79 - ETA: 20s - loss: 0.4616 - acc: 0.79 - ETA: 20s - loss: 0.4580 - acc: 0.79 - ETA: 19s - loss: 0.4543 - acc: 0.79 - ETA: 19s - loss: 0.4481 - acc: 0.80 - ETA: 19s - loss: 0.4457 - acc: 0.80 - ETA: 19s - loss: 0.4413 - acc: 0.80 - ETA: 18s - loss: 0.4374 - acc: 0.80 - ETA: 18s - loss: 0.4331 - acc: 0.81 - ETA: 18s - loss: 0.4287 - acc: 0.81 - ETA: 17s - loss: 0.4249 - acc: 0.81 - ETA: 17s - loss: 0.4224 - acc: 0.81 - ETA: 17s - loss: 0.4193 - acc: 0.82 - ETA: 16s - loss: 0.4165 - acc: 0.82 - ETA: 16s - loss: 0.4157 - acc: 0.82 - ETA: 16s - loss: 0.4104 - acc: 0.82 - ETA: 16s - loss: 0.4060 - acc: 0.82 - ETA: 15s - loss: 0.4023 - acc: 0.83 - ETA: 15s - loss: 0.4048 - acc: 0.82 - ETA: 15s - loss: 0.4020 - acc: 0.83 - ETA: 15s - loss: 0.4007 - acc: 0.83 - ETA: 14s - loss: 0.3977 - acc: 0.83 - ETA: 14s - loss: 0.4011 - acc: 0.83 - ETA: 14s - loss: 0.3997 - acc: 0.83 - ETA: 14s - loss: 0.3981 - acc: 0.83 - ETA: 13s - loss: 0.3976 - acc: 0.83 - ETA: 13s - loss: 0.3959 - acc: 0.83 - ETA: 13s - loss: 0.3929 - acc: 0.83 - ETA: 13s - loss: 0.3897 - acc: 0.83 - ETA: 12s - loss: 0.3872 - acc: 0.83 - ETA: 12s - loss: 0.3842 - acc: 0.84 - ETA: 12s - loss: 0.3819 - acc: 0.84 - ETA: 12s - loss: 0.3805 - acc: 0.84 - ETA: 11s - loss: 0.3769 - acc: 0.84 - ETA: 11s - loss: 0.3740 - acc: 0.84 - ETA: 11s - loss: 0.3722 - acc: 0.84 - ETA: 11s - loss: 0.3682 - acc: 0.84 - ETA: 10s - loss: 0.3684 - acc: 0.84 - ETA: 10s - loss: 0.3663 - acc: 0.84 - ETA: 10s - loss: 0.3653 - acc: 0.84 - ETA: 10s - loss: 0.3640 - acc: 0.85 - ETA: 9s - loss: 0.3622 - acc: 0.8517 - ETA: 9s - loss: 0.3615 - acc: 0.852 - ETA: 9s - loss: 0.3601 - acc: 0.853 - ETA: 9s - loss: 0.3575 - acc: 0.854 - ETA: 8s - loss: 0.3562 - acc: 0.855 - ETA: 8s - loss: 0.3539 - acc: 0.856 - ETA: 8s - loss: 0.3517 - acc: 0.857 - ETA: 8s - loss: 0.3513 - acc: 0.858 - ETA: 7s - loss: 0.3519 - acc: 0.858 - ETA: 7s - loss: 0.3496 - acc: 0.859 - ETA: 7s - loss: 0.3470 - acc: 0.861 - ETA: 7s - loss: 0.3451 - acc: 0.861 - ETA: 6s - loss: 0.3429 - acc: 0.862 - ETA: 6s - loss: 0.3410 - acc: 0.863 - ETA: 6s - loss: 0.3387 - acc: 0.864 - ETA: 6s - loss: 0.3392 - acc: 0.863 - ETA: 6s - loss: 0.3370 - acc: 0.864 - ETA: 5s - loss: 0.3347 - acc: 0.865 - ETA: 5s - loss: 0.3335 - acc: 0.867 - ETA: 5s - loss: 0.3314 - acc: 0.867 - ETA: 5s - loss: 0.3292 - acc: 0.868 - ETA: 4s - loss: 0.3301 - acc: 0.868 - ETA: 4s - loss: 0.3285 - acc: 0.869 - ETA: 4s - loss: 0.3287 - acc: 0.869 - ETA: 4s - loss: 0.3272 - acc: 0.870 - ETA: 3s - loss: 0.3270 - acc: 0.870 - ETA: 3s - loss: 0.3253 - acc: 0.871 - ETA: 3s - loss: 0.3232 - acc: 0.872 - ETA: 3s - loss: 0.3212 - acc: 0.873 - ETA: 3s - loss: 0.3206 - acc: 0.873 - ETA: 2s - loss: 0.3202 - acc: 0.874 - ETA: 2s - loss: 0.3196 - acc: 0.875 - ETA: 2s - loss: 0.3180 - acc: 0.875 - ETA: 2s - loss: 0.3164 - acc: 0.876 - ETA: 1s - loss: 0.3156 - acc: 0.876 - ETA: 1s - loss: 0.3151 - acc: 0.876 - ETA: 1s - loss: 0.3142 - acc: 0.876 - ETA: 1s - loss: 0.3151 - acc: 0.876 - ETA: 0s - loss: 0.3147 - acc: 0.877 - ETA: 0s - loss: 0.3132 - acc: 0.877 - ETA: 0s - loss: 0.3120 - acc: 0.878 - ETA: 0s - loss: 0.3110 - acc: 0.878 - ETA: 0s - loss: 0.3097 - acc: 0.878 - 28s 7ms/step - loss: 0.3092 - acc: 0.8790\n",
      "Epoch 2/5\n",
      "4008/4008 [==============================] - ETA: 26s - loss: 0.0744 - acc: 0.93 - ETA: 26s - loss: 0.0602 - acc: 0.96 - ETA: 26s - loss: 0.0569 - acc: 0.97 - ETA: 25s - loss: 0.0635 - acc: 0.97 - ETA: 25s - loss: 0.0558 - acc: 0.98 - ETA: 25s - loss: 0.0692 - acc: 0.96 - ETA: 25s - loss: 0.0618 - acc: 0.97 - ETA: 25s - loss: 0.0647 - acc: 0.96 - ETA: 24s - loss: 0.0607 - acc: 0.97 - ETA: 24s - loss: 0.0578 - acc: 0.97 - ETA: 24s - loss: 0.0550 - acc: 0.97 - ETA: 24s - loss: 0.0557 - acc: 0.97 - ETA: 24s - loss: 0.0537 - acc: 0.97 - ETA: 23s - loss: 0.0548 - acc: 0.97 - ETA: 23s - loss: 0.0529 - acc: 0.97 - ETA: 23s - loss: 0.0506 - acc: 0.97 - ETA: 23s - loss: 0.0516 - acc: 0.97 - ETA: 22s - loss: 0.0510 - acc: 0.97 - ETA: 22s - loss: 0.0571 - acc: 0.97 - ETA: 22s - loss: 0.0604 - acc: 0.97 - ETA: 22s - loss: 0.0619 - acc: 0.97 - ETA: 22s - loss: 0.0611 - acc: 0.97 - ETA: 21s - loss: 0.0606 - acc: 0.97 - ETA: 21s - loss: 0.0628 - acc: 0.97 - ETA: 21s - loss: 0.0611 - acc: 0.97 - ETA: 21s - loss: 0.0600 - acc: 0.97 - ETA: 21s - loss: 0.0642 - acc: 0.97 - ETA: 20s - loss: 0.0663 - acc: 0.97 - ETA: 20s - loss: 0.0645 - acc: 0.97 - ETA: 20s - loss: 0.0660 - acc: 0.97 - ETA: 20s - loss: 0.0686 - acc: 0.97 - ETA: 20s - loss: 0.0670 - acc: 0.97 - ETA: 19s - loss: 0.0661 - acc: 0.97 - ETA: 19s - loss: 0.0649 - acc: 0.97 - ETA: 19s - loss: 0.0640 - acc: 0.97 - ETA: 19s - loss: 0.0654 - acc: 0.97 - ETA: 18s - loss: 0.0662 - acc: 0.97 - ETA: 18s - loss: 0.0654 - acc: 0.97 - ETA: 18s - loss: 0.0687 - acc: 0.97 - ETA: 18s - loss: 0.0674 - acc: 0.97 - ETA: 18s - loss: 0.0667 - acc: 0.97 - ETA: 17s - loss: 0.0657 - acc: 0.97 - ETA: 17s - loss: 0.0648 - acc: 0.97 - ETA: 17s - loss: 0.0639 - acc: 0.97 - ETA: 17s - loss: 0.0628 - acc: 0.97 - ETA: 17s - loss: 0.0628 - acc: 0.97 - ETA: 16s - loss: 0.0625 - acc: 0.97 - ETA: 16s - loss: 0.0625 - acc: 0.97 - ETA: 16s - loss: 0.0616 - acc: 0.97 - ETA: 16s - loss: 0.0608 - acc: 0.97 - ETA: 16s - loss: 0.0603 - acc: 0.97 - ETA: 15s - loss: 0.0623 - acc: 0.97 - ETA: 15s - loss: 0.0613 - acc: 0.97 - ETA: 15s - loss: 0.0606 - acc: 0.97 - ETA: 15s - loss: 0.0608 - acc: 0.97 - ETA: 14s - loss: 0.0623 - acc: 0.97 - ETA: 14s - loss: 0.0616 - acc: 0.97 - ETA: 14s - loss: 0.0609 - acc: 0.97 - ETA: 14s - loss: 0.0601 - acc: 0.97 - ETA: 14s - loss: 0.0610 - acc: 0.97 - ETA: 13s - loss: 0.0603 - acc: 0.97 - ETA: 13s - loss: 0.0605 - acc: 0.97 - ETA: 13s - loss: 0.0604 - acc: 0.97 - ETA: 13s - loss: 0.0604 - acc: 0.97 - ETA: 12s - loss: 0.0599 - acc: 0.97 - ETA: 12s - loss: 0.0596 - acc: 0.97 - ETA: 12s - loss: 0.0592 - acc: 0.97 - ETA: 12s - loss: 0.0606 - acc: 0.97 - ETA: 12s - loss: 0.0610 - acc: 0.97 - ETA: 11s - loss: 0.0616 - acc: 0.97 - ETA: 11s - loss: 0.0617 - acc: 0.97 - ETA: 11s - loss: 0.0614 - acc: 0.97 - ETA: 11s - loss: 0.0608 - acc: 0.97 - ETA: 11s - loss: 0.0619 - acc: 0.97 - ETA: 10s - loss: 0.0613 - acc: 0.97 - ETA: 10s - loss: 0.0609 - acc: 0.97 - ETA: 10s - loss: 0.0607 - acc: 0.97 - ETA: 10s - loss: 0.0629 - acc: 0.97 - ETA: 9s - loss: 0.0625 - acc: 0.9775 - ETA: 9s - loss: 0.0627 - acc: 0.977 - ETA: 9s - loss: 0.0631 - acc: 0.976 - ETA: 9s - loss: 0.0631 - acc: 0.976 - ETA: 9s - loss: 0.0628 - acc: 0.977 - ETA: 8s - loss: 0.0627 - acc: 0.976 - ETA: 8s - loss: 0.0627 - acc: 0.977 - ETA: 8s - loss: 0.0622 - acc: 0.977 - ETA: 8s - loss: 0.0617 - acc: 0.977 - ETA: 8s - loss: 0.0615 - acc: 0.977 - ETA: 7s - loss: 0.0620 - acc: 0.977 - ETA: 7s - loss: 0.0635 - acc: 0.976 - ETA: 7s - loss: 0.0639 - acc: 0.976 - ETA: 7s - loss: 0.0633 - acc: 0.976 - ETA: 6s - loss: 0.0648 - acc: 0.975 - ETA: 6s - loss: 0.0663 - acc: 0.975 - ETA: 6s - loss: 0.0664 - acc: 0.975 - ETA: 6s - loss: 0.0664 - acc: 0.975 - ETA: 6s - loss: 0.0660 - acc: 0.975 - ETA: 5s - loss: 0.0657 - acc: 0.975 - ETA: 5s - loss: 0.0658 - acc: 0.975 - ETA: 5s - loss: 0.0657 - acc: 0.975 - ETA: 5s - loss: 0.0659 - acc: 0.974 - ETA: 5s - loss: 0.0653 - acc: 0.975 - ETA: 4s - loss: 0.0647 - acc: 0.975 - ETA: 4s - loss: 0.0642 - acc: 0.975 - ETA: 4s - loss: 0.0639 - acc: 0.975 - ETA: 4s - loss: 0.0635 - acc: 0.976 - ETA: 3s - loss: 0.0632 - acc: 0.976 - ETA: 3s - loss: 0.0629 - acc: 0.976 - ETA: 3s - loss: 0.0625 - acc: 0.976 - ETA: 3s - loss: 0.0626 - acc: 0.976 - ETA: 3s - loss: 0.0625 - acc: 0.976 - ETA: 2s - loss: 0.0625 - acc: 0.976 - ETA: 2s - loss: 0.0635 - acc: 0.976 - ETA: 2s - loss: 0.0649 - acc: 0.976 - ETA: 2s - loss: 0.0649 - acc: 0.976 - ETA: 1s - loss: 0.0646 - acc: 0.976 - ETA: 1s - loss: 0.0643 - acc: 0.976 - ETA: 1s - loss: 0.0640 - acc: 0.976 - ETA: 1s - loss: 0.0641 - acc: 0.976 - ETA: 1s - loss: 0.0637 - acc: 0.976 - ETA: 0s - loss: 0.0634 - acc: 0.977 - ETA: 0s - loss: 0.0636 - acc: 0.976 - ETA: 0s - loss: 0.0633 - acc: 0.977 - ETA: 0s - loss: 0.0633 - acc: 0.977 - ETA: 0s - loss: 0.0634 - acc: 0.976 - 27s 7ms/step - loss: 0.0633 - acc: 0.9768\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008/4008 [==============================] - ETA: 26s - loss: 0.0093 - acc: 1.00 - ETA: 26s - loss: 0.0547 - acc: 0.98 - ETA: 27s - loss: 0.0380 - acc: 0.98 - ETA: 26s - loss: 0.0312 - acc: 0.99 - ETA: 26s - loss: 0.0278 - acc: 0.99 - ETA: 26s - loss: 0.0266 - acc: 0.99 - ETA: 25s - loss: 0.0246 - acc: 0.99 - ETA: 25s - loss: 0.0243 - acc: 0.99 - ETA: 25s - loss: 0.0270 - acc: 0.99 - ETA: 24s - loss: 0.0279 - acc: 0.99 - ETA: 24s - loss: 0.0261 - acc: 0.99 - ETA: 24s - loss: 0.0254 - acc: 0.99 - ETA: 24s - loss: 0.0241 - acc: 0.99 - ETA: 24s - loss: 0.0237 - acc: 0.99 - ETA: 23s - loss: 0.0223 - acc: 0.99 - ETA: 23s - loss: 0.0242 - acc: 0.99 - ETA: 23s - loss: 0.0233 - acc: 0.99 - ETA: 23s - loss: 0.0223 - acc: 0.99 - ETA: 22s - loss: 0.0215 - acc: 0.99 - ETA: 22s - loss: 0.0206 - acc: 0.99 - ETA: 22s - loss: 0.0201 - acc: 0.99 - ETA: 22s - loss: 0.0199 - acc: 0.99 - ETA: 22s - loss: 0.0195 - acc: 0.99 - ETA: 21s - loss: 0.0190 - acc: 0.99 - ETA: 21s - loss: 0.0187 - acc: 0.99 - ETA: 21s - loss: 0.0189 - acc: 0.99 - ETA: 21s - loss: 0.0183 - acc: 0.99 - ETA: 20s - loss: 0.0179 - acc: 0.99 - ETA: 20s - loss: 0.0192 - acc: 0.99 - ETA: 20s - loss: 0.0189 - acc: 0.99 - ETA: 20s - loss: 0.0185 - acc: 0.99 - ETA: 20s - loss: 0.0185 - acc: 0.99 - ETA: 19s - loss: 0.0180 - acc: 0.99 - ETA: 19s - loss: 0.0178 - acc: 0.99 - ETA: 19s - loss: 0.0177 - acc: 0.99 - ETA: 19s - loss: 0.0175 - acc: 0.99 - ETA: 19s - loss: 0.0175 - acc: 0.99 - ETA: 18s - loss: 0.0174 - acc: 0.99 - ETA: 18s - loss: 0.0173 - acc: 0.99 - ETA: 18s - loss: 0.0170 - acc: 0.99 - ETA: 18s - loss: 0.0170 - acc: 0.99 - ETA: 17s - loss: 0.0168 - acc: 0.99 - ETA: 17s - loss: 0.0164 - acc: 0.99 - ETA: 17s - loss: 0.0164 - acc: 0.99 - ETA: 17s - loss: 0.0163 - acc: 0.99 - ETA: 17s - loss: 0.0162 - acc: 0.99 - ETA: 16s - loss: 0.0167 - acc: 0.99 - ETA: 16s - loss: 0.0166 - acc: 0.99 - ETA: 16s - loss: 0.0168 - acc: 0.99 - ETA: 16s - loss: 0.0173 - acc: 0.99 - ETA: 16s - loss: 0.0178 - acc: 0.99 - ETA: 15s - loss: 0.0175 - acc: 0.99 - ETA: 15s - loss: 0.0181 - acc: 0.99 - ETA: 15s - loss: 0.0178 - acc: 0.99 - ETA: 15s - loss: 0.0182 - acc: 0.99 - ETA: 14s - loss: 0.0180 - acc: 0.99 - ETA: 14s - loss: 0.0179 - acc: 0.99 - ETA: 14s - loss: 0.0177 - acc: 0.99 - ETA: 14s - loss: 0.0175 - acc: 0.99 - ETA: 14s - loss: 0.0177 - acc: 0.99 - ETA: 13s - loss: 0.0175 - acc: 0.99 - ETA: 13s - loss: 0.0176 - acc: 0.99 - ETA: 13s - loss: 0.0174 - acc: 0.99 - ETA: 13s - loss: 0.0173 - acc: 0.99 - ETA: 12s - loss: 0.0171 - acc: 0.99 - ETA: 12s - loss: 0.0170 - acc: 0.99 - ETA: 12s - loss: 0.0171 - acc: 0.99 - ETA: 12s - loss: 0.0169 - acc: 0.99 - ETA: 12s - loss: 0.0167 - acc: 0.99 - ETA: 11s - loss: 0.0165 - acc: 0.99 - ETA: 11s - loss: 0.0169 - acc: 0.99 - ETA: 11s - loss: 0.0169 - acc: 0.99 - ETA: 11s - loss: 0.0168 - acc: 0.99 - ETA: 11s - loss: 0.0177 - acc: 0.99 - ETA: 10s - loss: 0.0181 - acc: 0.99 - ETA: 10s - loss: 0.0181 - acc: 0.99 - ETA: 10s - loss: 0.0181 - acc: 0.99 - ETA: 10s - loss: 0.0179 - acc: 0.99 - ETA: 9s - loss: 0.0177 - acc: 0.9953 - ETA: 9s - loss: 0.0180 - acc: 0.994 - ETA: 9s - loss: 0.0179 - acc: 0.995 - ETA: 9s - loss: 0.0177 - acc: 0.995 - ETA: 9s - loss: 0.0177 - acc: 0.995 - ETA: 8s - loss: 0.0175 - acc: 0.995 - ETA: 8s - loss: 0.0180 - acc: 0.995 - ETA: 8s - loss: 0.0178 - acc: 0.995 - ETA: 8s - loss: 0.0176 - acc: 0.995 - ETA: 8s - loss: 0.0175 - acc: 0.995 - ETA: 7s - loss: 0.0174 - acc: 0.995 - ETA: 7s - loss: 0.0173 - acc: 0.995 - ETA: 7s - loss: 0.0172 - acc: 0.995 - ETA: 7s - loss: 0.0171 - acc: 0.995 - ETA: 6s - loss: 0.0170 - acc: 0.995 - ETA: 6s - loss: 0.0171 - acc: 0.995 - ETA: 6s - loss: 0.0171 - acc: 0.995 - ETA: 6s - loss: 0.0170 - acc: 0.995 - ETA: 6s - loss: 0.0170 - acc: 0.995 - ETA: 5s - loss: 0.0170 - acc: 0.995 - ETA: 5s - loss: 0.0169 - acc: 0.995 - ETA: 5s - loss: 0.0169 - acc: 0.995 - ETA: 5s - loss: 0.0171 - acc: 0.995 - ETA: 5s - loss: 0.0169 - acc: 0.995 - ETA: 4s - loss: 0.0168 - acc: 0.995 - ETA: 4s - loss: 0.0168 - acc: 0.995 - ETA: 4s - loss: 0.0168 - acc: 0.995 - ETA: 4s - loss: 0.0175 - acc: 0.995 - ETA: 3s - loss: 0.0173 - acc: 0.995 - ETA: 3s - loss: 0.0172 - acc: 0.995 - ETA: 3s - loss: 0.0180 - acc: 0.995 - ETA: 3s - loss: 0.0179 - acc: 0.995 - ETA: 3s - loss: 0.0179 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 2s - loss: 0.0183 - acc: 0.995 - ETA: 2s - loss: 0.0182 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 1s - loss: 0.0180 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0179 - acc: 0.995 - ETA: 1s - loss: 0.0182 - acc: 0.995 - ETA: 1s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - ETA: 0s - loss: 0.0183 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0180 - acc: 0.995 - 27s 7ms/step - loss: 0.0183 - acc: 0.9950\n",
      "Epoch 4/5\n",
      "4008/4008 [==============================] - ETA: 27s - loss: 0.0236 - acc: 1.00 - ETA: 27s - loss: 0.0137 - acc: 1.00 - ETA: 26s - loss: 0.0143 - acc: 1.00 - ETA: 26s - loss: 0.0146 - acc: 1.00 - ETA: 26s - loss: 0.0123 - acc: 1.00 - ETA: 25s - loss: 0.0108 - acc: 1.00 - ETA: 25s - loss: 0.0102 - acc: 1.00 - ETA: 25s - loss: 0.0094 - acc: 1.00 - ETA: 25s - loss: 0.0090 - acc: 1.00 - ETA: 25s - loss: 0.0100 - acc: 1.00 - ETA: 24s - loss: 0.0113 - acc: 1.00 - ETA: 24s - loss: 0.0114 - acc: 1.00 - ETA: 24s - loss: 0.0110 - acc: 1.00 - ETA: 23s - loss: 0.0104 - acc: 1.00 - ETA: 23s - loss: 0.0105 - acc: 1.00 - ETA: 23s - loss: 0.0124 - acc: 0.99 - ETA: 23s - loss: 0.0117 - acc: 0.99 - ETA: 23s - loss: 0.0113 - acc: 0.99 - ETA: 22s - loss: 0.0110 - acc: 0.99 - ETA: 22s - loss: 0.0108 - acc: 0.99 - ETA: 22s - loss: 0.0103 - acc: 0.99 - ETA: 22s - loss: 0.0101 - acc: 0.99 - ETA: 21s - loss: 0.0115 - acc: 0.99 - ETA: 21s - loss: 0.0112 - acc: 0.99 - ETA: 21s - loss: 0.0108 - acc: 0.99 - ETA: 21s - loss: 0.0105 - acc: 0.99 - ETA: 21s - loss: 0.0115 - acc: 0.99 - ETA: 20s - loss: 0.0113 - acc: 0.99 - ETA: 20s - loss: 0.0109 - acc: 0.99 - ETA: 20s - loss: 0.0106 - acc: 0.99 - ETA: 20s - loss: 0.0103 - acc: 0.99 - ETA: 20s - loss: 0.0102 - acc: 0.99 - ETA: 19s - loss: 0.0099 - acc: 0.99 - ETA: 19s - loss: 0.0097 - acc: 0.99 - ETA: 19s - loss: 0.0096 - acc: 0.99 - ETA: 19s - loss: 0.0095 - acc: 0.99 - ETA: 18s - loss: 0.0092 - acc: 0.99 - ETA: 18s - loss: 0.0090 - acc: 0.99 - ETA: 18s - loss: 0.0089 - acc: 0.99 - ETA: 18s - loss: 0.0103 - acc: 0.99 - ETA: 18s - loss: 0.0108 - acc: 0.99 - ETA: 17s - loss: 0.0106 - acc: 0.99 - ETA: 17s - loss: 0.0108 - acc: 0.99 - ETA: 17s - loss: 0.0106 - acc: 0.99 - ETA: 17s - loss: 0.0130 - acc: 0.99 - ETA: 17s - loss: 0.0128 - acc: 0.99 - ETA: 16s - loss: 0.0125 - acc: 0.99 - ETA: 16s - loss: 0.0123 - acc: 0.99 - ETA: 16s - loss: 0.0121 - acc: 0.99 - ETA: 16s - loss: 0.0119 - acc: 0.99 - ETA: 15s - loss: 0.0117 - acc: 0.99 - ETA: 15s - loss: 0.0121 - acc: 0.99 - ETA: 15s - loss: 0.0121 - acc: 0.99 - ETA: 15s - loss: 0.0119 - acc: 0.99 - ETA: 15s - loss: 0.0127 - acc: 0.99 - ETA: 14s - loss: 0.0126 - acc: 0.99 - ETA: 14s - loss: 0.0124 - acc: 0.99 - ETA: 14s - loss: 0.0123 - acc: 0.99 - ETA: 14s - loss: 0.0122 - acc: 0.99 - ETA: 14s - loss: 0.0120 - acc: 0.99 - ETA: 13s - loss: 0.0118 - acc: 0.99 - ETA: 13s - loss: 0.0117 - acc: 0.99 - ETA: 13s - loss: 0.0116 - acc: 0.99 - ETA: 13s - loss: 0.0114 - acc: 0.99 - ETA: 12s - loss: 0.0113 - acc: 0.99 - ETA: 12s - loss: 0.0112 - acc: 0.99 - ETA: 12s - loss: 0.0111 - acc: 0.99 - ETA: 12s - loss: 0.0110 - acc: 0.99 - ETA: 12s - loss: 0.0109 - acc: 0.99 - ETA: 11s - loss: 0.0108 - acc: 0.99 - ETA: 11s - loss: 0.0106 - acc: 0.99 - ETA: 11s - loss: 0.0105 - acc: 0.99 - ETA: 11s - loss: 0.0104 - acc: 0.99 - ETA: 11s - loss: 0.0102 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 10s - loss: 0.0106 - acc: 0.99 - ETA: 10s - loss: 0.0105 - acc: 0.99 - ETA: 10s - loss: 0.0103 - acc: 0.99 - ETA: 9s - loss: 0.0103 - acc: 0.9964 - ETA: 9s - loss: 0.0102 - acc: 0.996 - ETA: 9s - loss: 0.0101 - acc: 0.996 - ETA: 9s - loss: 0.0103 - acc: 0.996 - ETA: 9s - loss: 0.0102 - acc: 0.996 - ETA: 8s - loss: 0.0101 - acc: 0.996 - ETA: 8s - loss: 0.0102 - acc: 0.996 - ETA: 8s - loss: 0.0101 - acc: 0.996 - ETA: 8s - loss: 0.0108 - acc: 0.996 - ETA: 8s - loss: 0.0109 - acc: 0.996 - ETA: 7s - loss: 0.0108 - acc: 0.996 - ETA: 7s - loss: 0.0108 - acc: 0.996 - ETA: 7s - loss: 0.0111 - acc: 0.995 - ETA: 7s - loss: 0.0110 - acc: 0.995 - ETA: 6s - loss: 0.0111 - acc: 0.996 - ETA: 6s - loss: 0.0110 - acc: 0.996 - ETA: 6s - loss: 0.0110 - acc: 0.996 - ETA: 6s - loss: 0.0110 - acc: 0.996 - ETA: 6s - loss: 0.0109 - acc: 0.996 - ETA: 5s - loss: 0.0111 - acc: 0.995 - ETA: 5s - loss: 0.0110 - acc: 0.995 - ETA: 5s - loss: 0.0109 - acc: 0.995 - ETA: 5s - loss: 0.0109 - acc: 0.996 - ETA: 5s - loss: 0.0108 - acc: 0.996 - ETA: 4s - loss: 0.0108 - acc: 0.996 - ETA: 4s - loss: 0.0109 - acc: 0.995 - ETA: 4s - loss: 0.0108 - acc: 0.995 - ETA: 4s - loss: 0.0110 - acc: 0.995 - ETA: 3s - loss: 0.0109 - acc: 0.995 - ETA: 3s - loss: 0.0110 - acc: 0.995 - ETA: 3s - loss: 0.0109 - acc: 0.995 - ETA: 3s - loss: 0.0113 - acc: 0.995 - ETA: 3s - loss: 0.0113 - acc: 0.995 - ETA: 2s - loss: 0.0112 - acc: 0.995 - ETA: 2s - loss: 0.0112 - acc: 0.995 - ETA: 2s - loss: 0.0113 - acc: 0.995 - ETA: 2s - loss: 0.0112 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0112 - acc: 0.995 - ETA: 1s - loss: 0.0111 - acc: 0.995 - ETA: 1s - loss: 0.0110 - acc: 0.995 - ETA: 1s - loss: 0.0109 - acc: 0.995 - ETA: 0s - loss: 0.0109 - acc: 0.995 - ETA: 0s - loss: 0.0108 - acc: 0.995 - ETA: 0s - loss: 0.0108 - acc: 0.995 - ETA: 0s - loss: 0.0115 - acc: 0.995 - ETA: 0s - loss: 0.0114 - acc: 0.995 - 27s 7ms/step - loss: 0.0114 - acc: 0.9955\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4008/4008 [==============================] - ETA: 26s - loss: 4.9426e-04 - acc: 1.00 - ETA: 25s - loss: 7.6510e-04 - acc: 1.00 - ETA: 26s - loss: 8.6095e-04 - acc: 1.00 - ETA: 25s - loss: 0.0013 - acc: 1.0000   - ETA: 25s - loss: 0.0015 - acc: 1.00 - ETA: 25s - loss: 0.0014 - acc: 1.00 - ETA: 24s - loss: 0.0013 - acc: 1.00 - ETA: 24s - loss: 0.0013 - acc: 1.00 - ETA: 24s - loss: 0.0024 - acc: 1.00 - ETA: 24s - loss: 0.0027 - acc: 1.00 - ETA: 24s - loss: 0.0025 - acc: 1.00 - ETA: 24s - loss: 0.0026 - acc: 1.00 - ETA: 23s - loss: 0.0028 - acc: 1.00 - ETA: 23s - loss: 0.0027 - acc: 1.00 - ETA: 23s - loss: 0.0027 - acc: 1.00 - ETA: 23s - loss: 0.0034 - acc: 1.00 - ETA: 22s - loss: 0.0040 - acc: 1.00 - ETA: 22s - loss: 0.0038 - acc: 1.00 - ETA: 22s - loss: 0.0037 - acc: 1.00 - ETA: 22s - loss: 0.0035 - acc: 1.00 - ETA: 21s - loss: 0.0034 - acc: 1.00 - ETA: 21s - loss: 0.0034 - acc: 1.00 - ETA: 21s - loss: 0.0056 - acc: 0.99 - ETA: 21s - loss: 0.0055 - acc: 0.99 - ETA: 21s - loss: 0.0053 - acc: 0.99 - ETA: 20s - loss: 0.0051 - acc: 0.99 - ETA: 20s - loss: 0.0050 - acc: 0.99 - ETA: 20s - loss: 0.0049 - acc: 0.99 - ETA: 20s - loss: 0.0047 - acc: 0.99 - ETA: 20s - loss: 0.0051 - acc: 0.99 - ETA: 19s - loss: 0.0049 - acc: 0.99 - ETA: 19s - loss: 0.0050 - acc: 0.99 - ETA: 19s - loss: 0.0048 - acc: 0.99 - ETA: 19s - loss: 0.0050 - acc: 0.99 - ETA: 19s - loss: 0.0049 - acc: 0.99 - ETA: 18s - loss: 0.0048 - acc: 0.99 - ETA: 18s - loss: 0.0047 - acc: 0.99 - ETA: 18s - loss: 0.0046 - acc: 0.99 - ETA: 18s - loss: 0.0047 - acc: 0.99 - ETA: 17s - loss: 0.0047 - acc: 0.99 - ETA: 17s - loss: 0.0046 - acc: 0.99 - ETA: 17s - loss: 0.0045 - acc: 0.99 - ETA: 17s - loss: 0.0045 - acc: 0.99 - ETA: 17s - loss: 0.0044 - acc: 0.99 - ETA: 16s - loss: 0.0043 - acc: 0.99 - ETA: 16s - loss: 0.0043 - acc: 0.99 - ETA: 16s - loss: 0.0043 - acc: 0.99 - ETA: 16s - loss: 0.0042 - acc: 0.99 - ETA: 16s - loss: 0.0041 - acc: 0.99 - ETA: 15s - loss: 0.0041 - acc: 0.99 - ETA: 15s - loss: 0.0041 - acc: 0.99 - ETA: 15s - loss: 0.0040 - acc: 0.99 - ETA: 15s - loss: 0.0040 - acc: 0.99 - ETA: 15s - loss: 0.0039 - acc: 0.99 - ETA: 14s - loss: 0.0038 - acc: 0.99 - ETA: 14s - loss: 0.0038 - acc: 0.99 - ETA: 14s - loss: 0.0038 - acc: 0.99 - ETA: 14s - loss: 0.0038 - acc: 0.99 - ETA: 14s - loss: 0.0038 - acc: 0.99 - ETA: 13s - loss: 0.0038 - acc: 0.99 - ETA: 13s - loss: 0.0038 - acc: 0.99 - ETA: 13s - loss: 0.0037 - acc: 0.99 - ETA: 13s - loss: 0.0037 - acc: 0.99 - ETA: 13s - loss: 0.0036 - acc: 0.99 - ETA: 12s - loss: 0.0036 - acc: 0.99 - ETA: 12s - loss: 0.0036 - acc: 0.99 - ETA: 12s - loss: 0.0036 - acc: 0.99 - ETA: 12s - loss: 0.0036 - acc: 0.99 - ETA: 11s - loss: 0.0035 - acc: 0.99 - ETA: 11s - loss: 0.0035 - acc: 0.99 - ETA: 11s - loss: 0.0035 - acc: 0.99 - ETA: 11s - loss: 0.0038 - acc: 0.99 - ETA: 11s - loss: 0.0038 - acc: 0.99 - ETA: 10s - loss: 0.0038 - acc: 0.99 - ETA: 10s - loss: 0.0038 - acc: 0.99 - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 10s - loss: 0.0037 - acc: 0.99 - ETA: 9s - loss: 0.0037 - acc: 0.9992 - ETA: 9s - loss: 0.0036 - acc: 0.999 - ETA: 9s - loss: 0.0036 - acc: 0.999 - ETA: 9s - loss: 0.0036 - acc: 0.999 - ETA: 9s - loss: 0.0035 - acc: 0.999 - ETA: 8s - loss: 0.0037 - acc: 0.999 - ETA: 8s - loss: 0.0038 - acc: 0.999 - ETA: 8s - loss: 0.0038 - acc: 0.999 - ETA: 8s - loss: 0.0038 - acc: 0.999 - ETA: 7s - loss: 0.0037 - acc: 0.999 - ETA: 7s - loss: 0.0037 - acc: 0.999 - ETA: 7s - loss: 0.0037 - acc: 0.999 - ETA: 7s - loss: 0.0036 - acc: 0.999 - ETA: 7s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 6s - loss: 0.0038 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0037 - acc: 0.999 - ETA: 5s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0036 - acc: 0.999 - ETA: 4s - loss: 0.0035 - acc: 0.999 - ETA: 4s - loss: 0.0042 - acc: 0.999 - ETA: 4s - loss: 0.0051 - acc: 0.998 - ETA: 3s - loss: 0.0051 - acc: 0.998 - ETA: 3s - loss: 0.0051 - acc: 0.998 - ETA: 3s - loss: 0.0050 - acc: 0.998 - ETA: 3s - loss: 0.0050 - acc: 0.998 - ETA: 3s - loss: 0.0050 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 2s - loss: 0.0049 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0048 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.998 - ETA: 1s - loss: 0.0047 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - ETA: 0s - loss: 0.0048 - acc: 0.999 - 27s 7ms/step - loss: 0.0048 - acc: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1430480b908>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003/1003 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31536366303979696"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122632106660251"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9312063808574277\n",
      "Wall time: 54.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(500, 2), random_state=1)\n",
    "clf.fit(csc_matrix(X_train), y_train)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 200)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-14cf014e97f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m    287\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \"\"\"\n\u001b[0;32m    952\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"coefs_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 953\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mdecision\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \"\"\"\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;31m# Make sure self.hidden_layer_sizes is a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    575\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 577\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 200)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    'base_score': 0.5\n",
    "    'booster': 'gbtree'\n",
    "    'colsample_bylevel': 1\n",
    "    'colsample_bytree':\n",
    "    'gamma': 0\n",
    "    'learning_rate': 0.1\n",
    "    'max_delta_step': 0\n",
    "    'max_depth': 3,\n",
    "    'min_child_weight': 1\n",
    "    'missing': None\n",
    "    'n_estimators': 100\n",
    "    'nthread': 1\n",
    "    'objective': 'binary:logistic'\n",
    "    'reg_alpha': 0\n",
    "    'reg_lambda': 1\n",
    "    'scale_pos_weight': 1\n",
    "    'seed': 0\n",
    "    'silent': 1\n",
    "    'subsample': 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1236\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1238\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1840\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1841\u001b[1;33m                 \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_exc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1842\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36m_get_exc_info\u001b[1;34m(self, exc_tuple)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1808\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-6108b5f63eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\\n\\nfeaturesets = (fs_words, fs_ngrams, fs_pos, fs_words_min, fs_ngrams_min, fs_tfidf_words, fs_words_ngrams, fs_words_pos, fs_words_min_pos, fs_ngrams_pos, fs_words_ngrams_pos, fs_w2v, fs_d2v)\\nfeatureparams = {  'fs_words': {'random_state': 1}, \\n               \\n                  #fs_ngrams': {'random_state': 1}, \\n                   #'fs_pos': {'random_state': 1}, \\n                   #'fs_words_min': {'random_state': 1}, \\n                   #'fs_ngrams_min': {'random_state': 1}, \\n                   #'fs_tfidf_words': {'random_state': 1}, \\n                   #'fs_words_ngrams': {'random_state': 1}, \\n                   #'fs_words_pos': {'random_state': 1}, \\n                   #'fs_words_min_pos': {'random_state': 1}, \\n                   #'fs_ngrams_pos': {'random_state': 1}, \\n                   #'fs_words_ngrams_pos':  {'solver': 'lbfgs', 'random_state': 1}, \\n                   #'fs_w2v': {'random_state': 1}, \\n                   #'fs_d2v': {'random_state': 1}\\n                    }\\n\\nscores = []\\nfor fs in featuresets:\\n    name = [k for k,v in locals().items() if v is fs][0]\\n    if not featureparams[name]:\\n            continue\\n    X_train, X_test, y_train, y_test = get_train_test(fs, test_size=0, random_state=1)\\n    try:\\n        param_test1 =  {\\n            'hidden_layer_sizes': [(5717, 2)],\\n        }\\n        gsearch1 = GridSearchCV(estimator = MLPClassifier(**featureparams[name]), \\n        param_grid = param_test1, n_jobs=1,iid=False, cv=2)\\n        gsearch1.estimator.get_params()\\n        gsearch1.fit(csc_matrix(np.array(X_train)), np.array(y_train))\\n        print(name, gsearch1.best_params_, gsearch1.best_score_)\\n        scores.append(pd.DataFrame(gsearch1.cv_results_))\\n    except Exception as e:\\n        print(e)\\n        print('Skipped', name)\\n        continue\\nscores = pd.concat(scores)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2166\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2167\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2168\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-63>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                 \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, exc_tuple)\u001b[0m\n\u001b[0;32m   1821\u001b[0m         \"\"\"\n\u001b[0;32m   1822\u001b[0m         \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_exc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_tuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_exception_only\u001b[1;34m(etype, value)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \"\"\"\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTracebackException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\traceback.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[0mlookup_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0mcapture_locals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapture_locals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m                 _seen=_seen)\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\traceback.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001b[0m\n\u001b[0;32m    495\u001b[0m         self.stack = StackSummary.extract(\n\u001b[0;32m    496\u001b[0m             \u001b[0mwalk_tb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlookup_lines\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m             capture_locals=capture_locals)\n\u001b[0m\u001b[0;32m    498\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# Capture now to permit freeing resources: only complication is in the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \"\"\"\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\josef\\anaconda3\\envs\\question_classification\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "featuresets = (fs_words, fs_ngrams, fs_pos, fs_words_min, fs_ngrams_min, fs_tfidf_words, fs_words_ngrams, fs_words_pos, fs_words_min_pos, fs_ngrams_pos, fs_words_ngrams_pos, fs_w2v, fs_d2v)\n",
    "featureparams = {  'fs_words': {'random_state': 1}, \n",
    "               \n",
    "                  #fs_ngrams': {'random_state': 1}, \n",
    "                   #'fs_pos': {'random_state': 1}, \n",
    "                   #'fs_words_min': {'random_state': 1}, \n",
    "                   #'fs_ngrams_min': {'random_state': 1}, \n",
    "                   #'fs_tfidf_words': {'random_state': 1}, \n",
    "                   #'fs_words_ngrams': {'random_state': 1}, \n",
    "                   #'fs_words_pos': {'random_state': 1}, \n",
    "                   #'fs_words_min_pos': {'random_state': 1}, \n",
    "                   #'fs_ngrams_pos': {'random_state': 1}, \n",
    "                   #'fs_words_ngrams_pos':  {'solver': 'lbfgs', 'random_state': 1}, \n",
    "                   #'fs_w2v': {'random_state': 1}, \n",
    "                   #'fs_d2v': {'random_state': 1}\n",
    "                    }\n",
    "\n",
    "scores = []\n",
    "for fs in featuresets:\n",
    "    name = [k for k,v in locals().items() if v is fs][0]\n",
    "    if not featureparams[name]:\n",
    "            continue\n",
    "    X_train, X_test, y_train, y_test = get_train_test(fs, test_size=0, random_state=1)\n",
    "    try:\n",
    "        param_test1 =  {\n",
    "            'hidden_layer_sizes': [(5717, 2)],\n",
    "        }\n",
    "        gsearch1 = GridSearchCV(estimator = MLPClassifier(**featureparams[name]), \n",
    "        param_grid = param_test1, n_jobs=1,iid=False, cv=2)\n",
    "        gsearch1.estimator.get_params()\n",
    "        gsearch1.fit(csc_matrix(np.array(X_train)), np.array(y_train))\n",
    "        print(name, gsearch1.best_params_, gsearch1.best_score_)\n",
    "        scores.append(pd.DataFrame(gsearch1.cv_results_))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Skipped', name)\n",
    "        continue\n",
    "scores = pd.concat(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.937604</td>\n",
       "      <td>1.582694</td>\n",
       "      <td>0.204060</td>\n",
       "      <td>0.192057</td>\n",
       "      <td>(100, 100, 100)</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 100, 100)}</td>\n",
       "      <td>0.893057</td>\n",
       "      <td>0.893812</td>\n",
       "      <td>0.893435</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999601</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.210455</td>\n",
       "      <td>2.887299</td>\n",
       "      <td>0.007502</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>(100, 100, 2)</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 100, 2)}</td>\n",
       "      <td>0.899042</td>\n",
       "      <td>0.889820</td>\n",
       "      <td>0.894431</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999601</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.822761</td>\n",
       "      <td>0.183867</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>(100, 100)</td>\n",
       "      <td>{'hidden_layer_sizes': (100, 100)}</td>\n",
       "      <td>0.892658</td>\n",
       "      <td>0.889820</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999601</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_hidden_layer_sizes                                   params  split0_test_score  split1_test_score  mean_test_score  std_test_score  rank_test_score  split0_train_score  split1_train_score  mean_train_score  std_train_score\n",
       "0      13.937604      1.582694         0.204060        0.192057          (100, 100, 100)  {'hidden_layer_sizes': (100, 100, 100)}           0.893057           0.893812         0.893435        0.000378                2                 1.0            0.999601            0.9998           0.0002\n",
       "1      18.210455      2.887299         0.007502        0.000500            (100, 100, 2)    {'hidden_layer_sizes': (100, 100, 2)}           0.899042           0.889820         0.894431        0.004611                1                 1.0            0.999601            0.9998           0.0002\n",
       "2      17.822761      0.183867         0.007002        0.001000               (100, 100)       {'hidden_layer_sizes': (100, 100)}           0.892658           0.889820         0.891239        0.001419                3                 1.0            0.999601            0.9998           0.0002"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5717"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fs_words['Feature'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
