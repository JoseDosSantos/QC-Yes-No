{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Module Import\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Classification.config as cfg\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "import spacy\n",
    "import pickle\n",
    "import time\n",
    "import operator\n",
    "from nltk.util import ngrams\n",
    "from copy import deepcopy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors, svm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Defining the string cleaner\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    #return string\n",
    "    clean_string = string.replace(u'\\xa0', u' ')\n",
    "    clean_string = re.sub(r'\\d+', ' number ', clean_string)\n",
    "    clean_string = re.sub(r'\\n', ' ', clean_string)\n",
    "    clean_string = re.sub(r'Ä', 'Ae', clean_string)\n",
    "    clean_string = re.sub(r'ä', 'ae', clean_string)\n",
    "    clean_string = re.sub(r'Ö', 'Oe', clean_string)\n",
    "    clean_string = re.sub(r'ö', 'oe', clean_string)\n",
    "    clean_string = re.sub(r'Ü', 'Ue', clean_string)\n",
    "    clean_string = re.sub(r'ü', 'ue', clean_string)\n",
    "    clean_string = re.sub(r'ß', 'ss', clean_string)\n",
    "    clean_string = re.sub(r'°', ' Grad ', clean_string)\n",
    "    clean_string = re.sub(r'24/7', 'immer', clean_string)\n",
    "    clean_string = re.sub(r'/', ' ', clean_string)\n",
    "    clean_string = re.sub(r'%', ' Prozent ', clean_string)\n",
    "    clean_string = re.sub(r'[Zz][Bb]', 'zum Beispiel', clean_string)\n",
    "    clean_string = re.sub(r'[Dd][Hh]', 'das heißt', clean_string)\n",
    "    clean_string = re.sub(r'[Bb][Ss][Pp][Ww]', 'beispielsweise', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]allo', '', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]i', '', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]ey', '', clean_string)\n",
    "    clean_string = re.sub(r'[Gg]uten\\s[Mm]orgen', '', clean_string)\n",
    "    clean_string = re.sub(r'[Gg]uten\\s[Aa]bend', '', clean_string)\n",
    "    \n",
    "    clean_string = re.sub(r'(\\([^)]*\\))', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\"', '', clean_string)\n",
    "    clean_string = re.sub(r'\\+', '', clean_string)\n",
    "    clean_string = re.sub(r'-', '', clean_string)\n",
    "    clean_string = re.sub(r',', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\^', '', clean_string)\n",
    "    clean_string = re.sub(r'\\'', '', clean_string)\n",
    "    clean_string = re.sub(r'`', '', clean_string)\n",
    "    clean_string = re.sub(r'´', '', clean_string)\n",
    "\n",
    "\n",
    "    clean_string = re.sub(r'\\'', '', clean_string)\n",
    "    clean_string = re.sub(r'\\.', '', clean_string)\n",
    "    clean_string = re.sub(r'\\s{2,}', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\s(?=\\?)', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\?*(?=(?:\\?))', '', clean_string)\n",
    "    clean_string = clean_string.strip()\n",
    "    #bitte, danke, und, eigentlich, überhaupt, git, wirklich\n",
    "    return clean_string#.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Building the lemmatizer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Importing the dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30/30 Imported file C:\\Users\\Josef\\PycharmProjects\\QC-Yes-No\\Corpus\\Tweets\\questions\\output_1000_tweets_2018-09-22_17-18-48\\classified.csv. Total length: 5011)\n",
      "\n",
      "Import finished. Total length of data set: 5011\n",
      "Time taken: 37.56443214416504\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "data_set = []\n",
    "data_set_index = pd.DataFrame(columns=['File Name', 'Start', 'End', 'Size'])\n",
    "start_index = 0\n",
    "\n",
    "for i, file in enumerate(cfg.ALL_FILES):\n",
    "    file_count = len(cfg.ALL_FILES)\n",
    "    reader = csv.reader(open(file, 'r'), delimiter=';')\n",
    "\n",
    "    for line in reader:\n",
    "        try:\n",
    "            data_set.append([lemmatizer(clean(line[0])).lower(), line[1]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    print(('({0}/{1} Imported file {2}. Total length: {3})'.format(i + 1, file_count, file, len(data_set))), end='\\r')\n",
    "    \n",
    "    data_set_index.loc[i] = [file, start_index, len(data_set) - 1, len(data_set) - 1 - start_index]\n",
    "    start_index = len(data_set)\n",
    "\n",
    "print('\\n\\nImport finished. Total length of data set: {}'.format(len(data_set)))\n",
    "\n",
    "data_set_frame = pd.DataFrame(data_set, columns =['Feature', 'Label'])\n",
    "\n",
    "print('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Creating PosTag feature\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 41.63926982879639\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with open('..\\\\Classification\\\\nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)\n",
    "    \n",
    "data_set_frame['PosTags'] = ''\n",
    "\n",
    "for i, line in enumerate(data_set_frame['Feature']):\n",
    "    sent = nltk.tokenize.WordPunctTokenizer().tokenize(line)\n",
    "    tag_line = []\n",
    "    for tag in tagger.tag(sent):\n",
    "        tag_line.append(tag[1])\n",
    "    data_set_frame.at[i, 'PosTags'] = tag_line\n",
    "    #print(('Tagged line {}'.format(i)), end='\\r')\n",
    "\n",
    "print('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Defining the Classifier functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'algorithm': 'auto'\n",
    " - 'leaf_size': 30\n",
    " - 'metric': 'minkowski'\n",
    " - 'metric_params': None\n",
    " - 'n_jobs': 1\n",
    " - 'n_neighbors': 5\n",
    " - 'p': 2\n",
    " - 'weights': 'uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    kn_clf = neighbors.KNeighborsClassifier(**parameters)\n",
    "    kn_clf.fit(X_word_train, y_word_train)\n",
    "    kn_accuracy = kn_clf.score(X_word_test, y_word_test)\n",
    "    return kn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'alpha': 1.0\n",
    " - 'class_prior': None\n",
    " - 'fit_prior': True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    nb_clf = MultinomialNB(**parameters)\n",
    "    nb_clf.fit(X_word_train, y_word_train)\n",
    "    nb_accuracy = nb_clf.score(X_word_test, y_word_test)\n",
    "    return nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'class_weight': None\n",
    " - 'criterion': 'gini'\n",
    " - 'max_depth': None\n",
    " - 'max_features': None\n",
    " - 'max_leaf_nodes': None\n",
    " - 'min_impurity_decrease': 0.0\n",
    " - 'min_impurity_split': None\n",
    " - 'min_samples_leaf': 1\n",
    " - 'min_samples_split': 2\n",
    " - 'min_weight_fraction_leaf': 0.0\n",
    " - 'presort': False\n",
    " - 'random_state': None,\n",
    " - 'splitter': 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    dt_clf = DecisionTreeClassifier(**parameters)\n",
    "    dt_clf.fit(X_word_train, y_word_train)\n",
    "    dt_accuracy = dt_clf.score(X_word_test, y_word_test)\n",
    "    return dt_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'C': 1.0\n",
    " - 'cache_size': 200\n",
    " - 'class_weight': None\n",
    " - 'coef0': 0.0\n",
    " - 'decision_function_shape': 'ovr'\n",
    " - 'degree': 3\n",
    " - 'gamma': 'auto'\n",
    " - 'kernel': 'rbf'\n",
    " - 'max_iter': -1\n",
    " - 'probability': False\n",
    " - 'random_state': None\n",
    " - 'shrinking': True\n",
    " - 'tol': 0.001\n",
    " - 'verbose': False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    svm_clf = svm.SVC(**parameters)\n",
    "    svm_clf.fit(X_train, y_train)\n",
    "    svm_accuracy = svm_clf.score(X_test, y_test)\n",
    "    return svm_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default Parameters:__\n",
    " - 'base_score': 0.5\n",
    " - 'booster': 'gbtree'\n",
    " - 'colsample_bylevel': 1\n",
    " - 'colsample_bytree': \n",
    " - 'gamma': 0\n",
    " - 'learning_rate': 0.1\n",
    " - 'max_delta_step': 0\n",
    " - 'max_depth': 3, \n",
    " - 'min_child_weight': 1\n",
    " - 'missing': None\n",
    " - 'n_estimators': 100\n",
    " - 'nthread': 1\n",
    " - 'objective': 'binary:logistic'\n",
    " - 'reg_alpha': 0\n",
    " - 'reg_lambda': 1\n",
    " - 'scale_pos_weight': 1\n",
    " - 'seed': 0\n",
    " - 'silent': 1 \n",
    " - 'subsample': 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XG_Boost(X_train, X_test, y_train, y_test, parameters = {}):\n",
    "    xg_clf = xgb.XGBClassifier(**parameters)\n",
    "    xg_clf.fit(X_train, y_train)\n",
    "    xg_accuracy = xg_clf.score(X_test, y_test)\n",
    "    return xg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Defining the train-test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(feature_set, test_size=0.2):\n",
    "    X, y = feature_set.iloc[:,:-1], feature_set.iloc[:,-1]\n",
    "    X_frame = np.array(X[0].tolist())\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_frame, y, test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Defining the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_frame, predictions):\n",
    "    with open('checked_list.pickle', 'rb') as f:\n",
    "        checked_rows = pickle.load(f)\n",
    "    \n",
    "    test_frame_local = deepcopy(test_frame)\n",
    "    test_frame_local.columns = ['Label']\n",
    "    test_frame_local['Prediction'] = preds\n",
    "    false_rows = []\n",
    "    \n",
    "    def find_dataset_location(index):\n",
    "        found = data_set_index.loc[(data_set_index['Start'] <= index) & (data_set_index['End'] >= index)]\n",
    "        found_info = found['File Name'].item().split('\\\\Corpus\\\\')[1]\n",
    "        found_type, found_file = found_info.split('\\\\')[0], found_info.split('\\\\')[2]\n",
    "        index_in_file = index - found['Start'].item() + 1\n",
    "        return found_type, found_file, index_in_file\n",
    "    \n",
    "    \n",
    "    for index, row in test_frame_local.iterrows():\n",
    "        i=1\n",
    "        if row['Prediction'] != row['Label']:\n",
    "            i+=1\n",
    "            loc = find_dataset_location(index)\n",
    "            false_rows.append([index, data_set[index][0], row['Label'], row['Prediction'], loc[0], loc[1], loc[2]])\n",
    "            \n",
    "    false_rows.sort(key=operator.itemgetter(0))\n",
    "    evaluation = pd.DataFrame(false_rows, columns=['Idx', 'Question', 'Label', 'Pred', 'Type', 'File', 'Line in File'])\n",
    "    display('Found {0} possibly wrongly labeled questions.'.format(len(evaluation.loc[~evaluation['Idx'].isin(checked_rows)])))\n",
    "    display(evaluation.loc[~evaluation['Idx'].isin(checked_rows)])\n",
    "    checked_rows.update(evaluation['Idx'].values)\n",
    "\n",
    "    with open('checked_list.pickle', 'wb') as f:\n",
    "        pickle.dump(checked_rows, f, protocol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building the feature bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of words. Amount of words: 5960\n",
      "Created bag of ngrams. Amount of ngrams: 19582\n",
      "Created bag of tags. Amount of tags: 45\n",
      "Time taken: 0.37308526039123535\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tags_bag = set(tag for index, row in data_set_frame.iterrows() for tag in row['PosTags'])\n",
    "print('Created bag of tags. Amount of tags: {0}'.format(len(tags_bag)))\n",
    "\n",
    "print('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Creating words based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of words. Amount of words: 5960\n",
      "Encoded bag of words feature set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Time taken:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "151.46939134597778"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bag_words = set(word for passage in data_set for word in nltk.tokenize.WordPunctTokenizer().tokenize(passage[0]))\n",
    "print('Created bag of words. Amount of words: {0}'.format(len(words_bag)))\n",
    "\n",
    "fs_words= pd.DataFrame([([(word in nltk.tokenize.WordPunctTokenizer().tokenize(data[0])) for word in bag_words], data[1]) for data in data_set])\n",
    "print('Encoded bag of words feature set.')\n",
    "\n",
    "display('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Getting data points for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time taken:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3710825443267822"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "X_word_train, X_word_test, y_word_train, y_word_test = get_train_test(fs_words, 0.2)\n",
    "\n",
    "display('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####          Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "0.8554336989032901\n"
     ]
    }
   ],
   "source": [
    "parameters = {}\n",
    "print(naive_bayes(X_word_train, X_word_test, y_word_train, y_word_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "0.9481555333998006\n"
     ]
    }
   ],
   "source": [
    "parameters = {}\n",
    "print(decision_tree(X_word_train, X_word_test, y_word_train, y_word_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-nearest-Neigbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8325024925224327\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_jobs' : -1}\n",
    "print(k_nearest(X_word_train, X_word_test, y_word_train, y_word_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "parameters = {'kernel' : 'linear'}\n",
    "print(SVM(X_word_train, X_word_test, y_word_train, y_word_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9391824526420738\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth' : 5, 'n_estimators' : 100, 'learning_rate' : 0.125, 'min_child_weight' : 1, 'njobs' : -1}\n",
    "print(XG_Boost(X_word_train, X_word_test, y_word_train, y_word_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Bag of ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6.1 Creating bigrams of words based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bag of ngrams. Amount of ngrams: 19582\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "bag_ngrams = set(gram for passage in data_set for gram in ngrams(nltk.tokenize.WordPunctTokenizer().tokenize(passage[0]), 2))\n",
    "print('Created bag of ngrams. Amount of ngrams: {0}'.format(len(ngrams_bag)))\n",
    "\n",
    "fs_ngrams= pd.DataFrame([([(gram in ngrams(nltk.tokenize.WordPunctTokenizer().tokenize(data[0]), 2)) for gram in ngrams_bag], data[1]) for data in data_set])\n",
    "print('Encoded bag of words feature set.')\n",
    "\n",
    "display('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Creating data points for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "x_ngram_train, x_ngram_test, y_ngram_train, y_ngram_test = get_train_test(fs_ngrams, 0.2)\n",
    "\n",
    "display('Time taken:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Training and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####          Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "0.8554336989032901\n"
     ]
    }
   ],
   "source": [
    "parameters = {}\n",
    "print(naive_bayes(X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': False, 'random_state': None, 'splitter': 'best'}\n",
      "0.9481555333998006\n"
     ]
    }
   ],
   "source": [
    "parameters = {}\n",
    "print(decision_tree(X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-nearest-Neigbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8325024925224327\n"
     ]
    }
   ],
   "source": [
    "parameters = {'n_jobs' : -1}\n",
    "print(k_nearest(X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "parameters = {'kernel' : 'linear'}\n",
    "print(SVM(X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9391824526420738\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth' : 6, 'n_estimators' : 200, 'learning_rate' : 0.15, 'min_child_weight' : 1, 'njobs' : -1}\n",
    "print(XG_Boost(X_ngram_train, X_ngram_test, y_ngram_train, y_ngram_test, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Creating postags based feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Classification\\\\nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)\n",
    "    \n",
    "data_set_frame = pd.DataFrame(data_set)\n",
    "data_set_frame['PosTags'] = ''\n",
    "for i, line in enumerate(data_set_frame[0]):\n",
    "    sent = nltk.tokenize.WordPunctTokenizer().tokenize(line)\n",
    "    tag_line = []\n",
    "    for tag in tagger.tag(sent):\n",
    "        tag_line.append(tag[1])\n",
    "    data_set_frame.at[i, 'PosTags'] = tag_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bag = set(tag for index, row in data_set_frame.iterrows() for tag in row['PosTags'])\n",
    "fs_pos= [([(tag in row['PosTags']) for tag in tag_bag], row[1]) for item, row in data_set_frame.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_pos = pd.DataFrame(fs_pos)\n",
    "x_pos, y_pos = bag_pos.iloc[:,:-1],bag_pos.iloc[:,-1]\n",
    "x_frame_pos = pd.DataFrame(x_pos[0].tolist(), columns = tag_bag)\n",
    "x_pos_train, x_pos_test, y_pos_train, y_pos_test = train_test_split(x_frame_pos, y_pos, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_class_pos = xgb.XGBClassifier(max_depth=6, n_estimators=125, learning_rate=0.125, min_child_weight = 1, njobs=4)\n",
    "xg_class_pos.fit(x_pos_train, y_pos_train)\n",
    "preds = xg_class_pos.predict(x_pos_test)\n",
    "accuracy_score(preds, y_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_frame_com = pd.concat([x_frame_pos, x_frame_word], axis=1)\n",
    "x_com_train, x_com_test, y_com_train, y_com_test = train_test_split(x_frame_com, y_pos, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_class_com = xgb.XGBClassifier(max_depth=6, n_estimators=125, learning_rate=0.125, min_child_weight = 0.5, subsample = 0.5, njobs=4)\n",
    "xg_class_com.fit(x_com_train, y_com_train)\n",
    "preds = xg_class_com.predict(x_com_test)\n",
    "accuracy_score(preds, y_com_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = pd.DataFrame(y_com_test)\n",
    "test_frame.columns=['Label']\n",
    "test_frame['Prediction']=preds\n",
    "for index, row in test_frame.iterrows():\n",
    "    if row['Prediction'] != row['Label']:\n",
    "        print(data_set[index], 'Label:', row['Label'], 'Prediction:', row['Prediction'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds\n",
    "wrong = [(index, y_com_test[index])  for index in preds if preds[index] != y_com_test[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_2 = [(index, test['Pred'][index])  for index in test.index if test[1][index] != test['Pred'][index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "all_lists = list(pd.DataFrame(data_set)[0])\n",
    "test_string = ''\n",
    "\n",
    "for e,i in enumerate(all_lists):\n",
    "    test_string += ' '\n",
    "    test_string += i\n",
    "    if i.split().count('?') < 1:\n",
    "        print(i, e)\n",
    "test_string = test_string.split()\n",
    "\n",
    "for word in words_bag:\n",
    "    count[word] = test_string.count(word)\n",
    "import operator\n",
    "sorted_x = sorted(count.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(sorted_x)\n",
    "frame.set_index(0, inplace=True)\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "test_frame = deepcopy(data_set_frame)\n",
    "test_frame['Feature_min'] = ''\n",
    "for index, row in test_frame.iterrows():\n",
    "    for word in row[0].rsplit():\n",
    "        if frame[1][word] > 10:\n",
    "            test_frame.at[index, 'Feature_min'] += (word + ' ')\n",
    "        test_frame.at[index, 'Feature_min'] = test_frame.at[index, 'Feature_min']\n",
    "    print(('Checked row: {}'.format(index)), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bag_min = set(word for word in words_bag if frame[1][word] > 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_words_min= [([(word in data[2].split()) for word in words_bag_min], data[1]) for index, data in test_frame.iterrows()]\n",
    "bag_words_min = pd.DataFrame(fs_words_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_min, y_word_min = bag_words_min.iloc[:,:-1],bag_words_min.iloc[:,-1]\n",
    "x_frame_word_min = pd.DataFrame(x_word_min[0].tolist(), columns = words_bag_min)\n",
    "x_word_min_train, x_word_min_test, y_word_min_train, y_word_min_test = train_test_split(x_frame_word_min, y_word_min, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_class_word_min = xgb.XGBClassifier(max_depth=7, n_estimators=100, learning_rate=0.125, min_child_weight = 0.4, subsample=0.7, njobs=4)\n",
    "xg_class_word_min.fit(x_word_min_train, y_word_min_train)\n",
    "preds = xg_class_word_min.predict(x_word_min_test)\n",
    "accuracy_score(preds, y_word_min_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 =  {\n",
    " 'learning_rate':[0.125, 1.5],\n",
    " 'n_estimators':[75, 100],\n",
    " 'max_depth': [5, 6, 7],\n",
    " 'min_child_weight': [0.5, 1],\n",
    " 'subsample': [0.5, 1]}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.125,\n",
    "                                                      max_depth = 5,\n",
    "                                                      random_state= 10,\n",
    "                                                      min_child_weight = 1), \n",
    "param_grid = param_test1, n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.estimator.get_params()\n",
    "gsearch1.fit(np.array(x_word_min_train), np.array(y_word_min_train))\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(xg_class_word, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(30, 30))\n",
    "fig, ax = plt.subplots(figsize=(30, 30))\n",
    "xgb.plot_tree(xg_class_word, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 =  {\n",
    " 'learning_rate':[0.125, 1.5],\n",
    " 'n_estimators':[75, 100, 125],\n",
    " 'max_depth': [4, 5, 6]}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1,\n",
    "                                                      max_depth = 5,\n",
    "                                                      random_state= 10,\n",
    "                                                      min_child_weight = 1), \n",
    "param_grid = param_test1, n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.estimator.get_params()\n",
    "gsearch1.fit(np.array(x_word_train), np.array(y_word_train))\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x, y = bag_ngram.iloc[:,:-1],bag_ngram.iloc[:,-1]\n",
    "\n",
    "x_frame = pd.DataFrame(x[0].tolist(), columns = ngrams_bag)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_frame, y, test_size=0.2)#, random_state=20)\n",
    "\n",
    "xg_class = xgb.XGBClassifier(max_depth=6, n_estimators=100, learning_rate=0.1, min_child_weight = 1, njobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_class.fit(x_train, y_train)\n",
    "preds = xg_class.predict(x_test)\n",
    "accuracy_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_ngram = [([(gram in ngrams(nltk.tokenize.WordPunctTokenizer().tokenize(data[0]), 2)) for gram in ngrams_bag], data[1]) for data in data_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_ngram = pd.DataFrame(fs_ngram)\n",
    "\n",
    "x, y = bag_ngram.iloc[:,:-1],bag_ngram.iloc[:,-1]\n",
    "\n",
    "x_frame = pd.DataFrame(x[0].tolist(), columns = ngrams_bag)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_frame, y, test_size=0.2)#, random_state=20)\n",
    "\n",
    "xg_class = xgb.XGBClassifier(max_depth=7, n_estimators=400, learning_rate=0.1, min_child_weight = 1, njobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_class.fit(x_train, y_train)\n",
    "preds = xg_class.predict(x_test)\n",
    "accuracy_score(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_encoded = pd.DataFrame(fs_words)\n",
    "\n",
    "x, y = bag_encoded.iloc[:,:-1],bag_encoded.iloc[:,-1]\n",
    "\n",
    "x_frame = pd.DataFrame(x[0].tolist(), columns = words_bag)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "accuracy = []\n",
    "for i in range(15):\n",
    "    xg_class = xgb.XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.125, min_child_weight = 1, njobs=4)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_frame, y, test_size=0.2, random_state=i)\n",
    "    xg_class.fit(x_train, y_train)\n",
    "    preds = xg_class.predict(x_test)\n",
    "    accuracy.append(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 =  {\n",
    " 'learning_rate':[0.05, 0.1, 0.15],\n",
    " 'n_estimators':[75, 100, 125],\n",
    " 'max_depth': [6, 7],\n",
    " 'min_child_weight': [1, 2]}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1,\n",
    "                                                      max_depth = 5,\n",
    "                                                      random_state= 10,\n",
    "                                                      min_child_weight = 1), \n",
    "param_grid = param_test1, n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.estimator.get_params()\n",
    "gsearch1.fit(np.array(x_train), np.array(y_train))\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pred']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = [3,5, 10]\n",
    "esti = [50, 100, 300, 1000]\n",
    "learning_rate = [0.05, 0.2, 0.4]\n",
    "\n",
    "for d in depth:\n",
    "    for e in esti:\n",
    "        for l in learning_rate:\n",
    "            xg_class = xgb.XGBClassifier(max_depth=d, n_estimators=e, learning_rate=l)\n",
    "            xg_class.fit(x_train, y_train)\n",
    "            preds = xg_class.predict(x_test)\n",
    "            print('Depth:', d, 'Estimators:', e, 'Learning Rate:', l, 'Accuracy', accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds)*937799043062201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = [(index, test['Pred'][index])  for index in test.index if test[1][index] != test['Pred'][index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_2 = [(index, test['Pred'][index])  for index in test.index if test[1][index] != test['Pred'][index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in wrong:\n",
    "    print(i, data_set[i], 'pred:', pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
