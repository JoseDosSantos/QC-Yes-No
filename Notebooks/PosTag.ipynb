{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Classification.config as cfg\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "import warnings\n",
    "import spacy\n",
    "from nltk.util import ngrams\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from ClassifierBasedGermanTagger import ClassifierBasedGermanTagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(string):\n",
    "    #return string\n",
    "    clean_string = string.replace(u'\\xa0', u' ')\n",
    "    clean_string = re.sub(r'\\d+', 'number', clean_string)\n",
    "    clean_string = re.sub(r'Ä', 'Ae', clean_string)\n",
    "    clean_string = re.sub(r'ä', 'ae', clean_string)\n",
    "    clean_string = re.sub(r'Ö', 'Oe', clean_string)\n",
    "    clean_string = re.sub(r'ö', 'oe', clean_string)\n",
    "    clean_string = re.sub(r'Ü', 'Ue', clean_string)\n",
    "    clean_string = re.sub(r'ü', 'ue', clean_string)\n",
    "    clean_string = re.sub(r'ß', 'ss', clean_string)\n",
    "    clean_string = re.sub(r'°', 'Grad', clean_string)\n",
    "    clean_string = re.sub(r'[Zz][Bb]', 'zum Beispiel', clean_string)\n",
    "    clean_string = re.sub(r'[Dd][Hh]', 'das heißt', clean_string)\n",
    "    clean_string = re.sub(r'[Bb][Ss][Pp][Ww]', 'beispielsweise', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]allo', '', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]i', '', clean_string)\n",
    "    clean_string = re.sub(r'[Hh]ey', '', clean_string)\n",
    "    clean_string = re.sub(r'[Gg]uten\\s[Mm]orgen', '', clean_string)\n",
    "    clean_string = re.sub(r'[Gg]uten\\s[Aa]bend', '', clean_string)\n",
    "    \n",
    "    clean_string = re.sub(r'(\\([^)]*\\))', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\"', '', clean_string)\n",
    "    clean_string = re.sub(r'\\+', '', clean_string)\n",
    "    clean_string = re.sub(r'-', '', clean_string)\n",
    "    clean_string = re.sub(r',', '', clean_string)\n",
    "\n",
    "    clean_string = re.sub(r'\\'', '', clean_string)\n",
    "    clean_string = re.sub(r'\\.', '', clean_string)\n",
    "    clean_string = re.sub(r'\\s{2,}', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\s(?=\\?)', ' ', clean_string)\n",
    "    clean_string = re.sub(r'\\?*(?=(?:\\?))', '', clean_string)\n",
    "    clean_string = clean_string.strip()\n",
    "    try:\n",
    "        clean_string = clean_string.split(' ', 1)[0].capitalize() + ' ' + clean_string.split(' ', 1)[1]\n",
    "    except:\n",
    "        pass\n",
    "    #bitte, danke, und, eigentlich, überhaupt, git, wirklich\n",
    "    return clean_string#.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de')\n",
    "\n",
    "def lemmatizer(text):\n",
    "    sent = []\n",
    "    doc = nlp(text)\n",
    "    for word in doc:\n",
    "        sent.append(word.lemma_)\n",
    "    return \" \".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for file in cfg.ALL_FILES:\n",
    "    reader = csv.reader(open(file, 'r'), delimiter=';')\n",
    "    for line in reader:\n",
    "        try:\n",
    "            data_set.append([lemmatizer(clean(line[0])), line[1]])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(line)\n",
    "    #print(file)\n",
    "    #print(len(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bag = set(word for passage in data_set for word in nltk.tokenize.WordPunctTokenizer().tokenize(passage[0]))\n",
    "ngrams_bag = set(gram for passage in data_set for gram in ngrams(nltk.tokenize.WordPunctTokenizer().tokenize(passage[0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length dataset: 5012\n",
      "Size bag of words: 6305\n",
      "Size bag of ngrams: 19905\n"
     ]
    }
   ],
   "source": [
    "print('Length dataset:', len(data_set))\n",
    "print('Size bag of words:', len(words_bag))\n",
    "print('Size bag of ngrams:', len(ngrams_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_frame = pd.DataFrame(data_set)\n",
    "data_set_frame['PosTags'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [PPOSAT, ART, ADJA, APPR, APPR, ART, ADJA, NN,...\n",
       "1           [VMFIN, ART, ADJA, ADJA, ADV, ADV, VVINF, $.]\n",
       "2       [PPOSAT, ADV, PROAV, ADV, ART, NN, NN, VVINF, $.]\n",
       "3       [VVFIN, ART, NN, APPR, ART, NN, KON, APPR, ART...\n",
       "4                      [PWAV, ADJA, ADJA, ART, TRUNC, $.]\n",
       "5       [VAFIN, PIS, NN, KOKOM, ADJD, ART, NN, VVFIN, ...\n",
       "6                        [PWAV, ADJD, VAINF, ART, NN, $.]\n",
       "7       [PWAV, PPOSAT, ADV, ART, ADJD, NN, ART, NN, AP...\n",
       "8                       [PPOSAT, ART, NN, APPR, ADJA, $.]\n",
       "9       [VMFIN, ADV, ART, NN, ADV, APPR, ART, NN, APPR...\n",
       "10               [PWS, VVFIN, KOUS, ART, ADJA, VAINF, $.]\n",
       "11                     [VMFIN, PPER, ART, ADV, VVFIN, $.]\n",
       "12                 [PWAV, ADJD, VAINF, ART, NN, ADJD, $.]\n",
       "13                                [VAFIN, PIS, KOKOM, $.]\n",
       "14                 [PPOSAT, ART, NN, NE, VVPP, VAINF, $.]\n",
       "15        [PWAV, ADJD, VAINF, ART, NN, ADJD, ART, NN, $.]\n",
       "16      [PWAV, VMFIN, ART, PPOSAT, ART, PPER, NN, APPR...\n",
       "17                                [PPOSAT, NN, PROAV, $.]\n",
       "18                           [VVFIN, PPER, NN, PROAV, $.]\n",
       "19      [VMFIN, PIS, ART, ADJA, KON, ADV, APPR, NE, VV...\n",
       "20                            [PPOSAT, ART, NE, VVPP, $.]\n",
       "21            [VMFIN, PIS, PDS, ADV, APPR, NE, VVFIN, $.]\n",
       "22      [PWAV, PPOSAT, TRUNC, APPRART, NN, APPR, NE, V...\n",
       "23       [PWAV, VVFIN, ART, ADV, APPR, ART, NN, APPR, $.]\n",
       "24             [PWAV, VVFIN, ART, NN, APPR, ADJD, NN, $.]\n",
       "25      [VMFIN, PIS, ART, NN, ADV, APPR, ART, NN, VVIN...\n",
       "26          [PWAV, ADJD, PRF, ART, NN, APPR, ART, NN, $.]\n",
       "27      [APPR, PWAT, NN, VMFIN, ART, NN, VVFIN, VAINF,...\n",
       "28                 [PPOSAT, ART, NN, APPR, ART, ADJD, $.]\n",
       "29             [PWAV, PIAT, NN, KON, NN, VAFIN, PPER, $.]\n",
       "                              ...                        \n",
       "4982    [APPR, PWS, ADJD, VMFIN, PDS, NN, KON, VVINF, $.]\n",
       "4983              [PWAV, ADJD, PPOSAT, PPOSAT, VVINF, $.]\n",
       "4984                         [PWAV, VVFIN, PPER, ADV, $.]\n",
       "4985                                [PWAV, ADV, ADJD, $.]\n",
       "4986    [PWAV, VVFIN, ADV, PIAT, NN, NN, KON, NN, APPR...\n",
       "4987    [PWAV, VMFIN, ART, ADV, VVINF, KOKOM, ART, NN,...\n",
       "4988                      [PPOSAT, ART, ADV, ART, NN, $.]\n",
       "4989    [VAFIN, PPER, APPR, ART, ADV, CARD, NN, ART, N...\n",
       "4990                          [PWAV, PPOSAT, ART, NN, $.]\n",
       "4991               [PIAT, NN, VAINF, PPER, APPR, PRF, $.]\n",
       "4992       [PWS, VAFIN, ART, APPR, ADV, PTKZU, VVINF, $.]\n",
       "4993                  [PIAT, VVFIN, VVINF, PPER, ADV, $.]\n",
       "4994    [PWAV, VMFIN, ADV, ART, NN, ADJD, ADV, VVINF, ...\n",
       "4995                  [VMFIN, PPER, ADV, ADJD, VVINF, $.]\n",
       "4996    [APPR, PIAT, NN, VVINF, PPER, PRF, APPRART, AD...\n",
       "4997                   [PPOSAT, PPER, ADV, ADV, ADJD, $.]\n",
       "4998       [VMFIN, PPOSAT, PRF, APPR, PDS, NN, VVINF, $.]\n",
       "4999    [PIAT, PPER, PRF, APPR, ADV, ADV, NN, PTKZU, N...\n",
       "5000    [APPR, PWAV, VMFIN, PIS, VVFIN, APPR, NE, VAFI...\n",
       "5001    [PWS, VAFIN, PIAT, PIAT, NN, KON, VMFIN, PROAV...\n",
       "5002            [KON, PWAV, VVFIN, PPOSAT, ART, ADJD, $.]\n",
       "5003                               [PPOSAT, ART, ADV, $.]\n",
       "5004                       [PPOSAT, NN, ADV, ART, NN, $.]\n",
       "5005    [NE, PPOSAT, APPR, ADJD, VVINF, KON, PPOSAT, A...\n",
       "5006    [PWAV, VVFIN, PPER, PPOSAT, NN, APPR, ADJD, KO...\n",
       "5007                   [PWS, ADV, NN, VAFIN, ART, NN, $.]\n",
       "5008                      [NE, PPER, APPR, PRF, ADJD, $.]\n",
       "5009      [PWAV, ADJD, PPOSAT, ART, ADV, ADJD, VVINF, $.]\n",
       "5010                             [PIAT, PPOSAT, PPER, $.]\n",
       "5011                        [PWAV, ADJD, VAINF, PPER, $.]\n",
       "Name: PosTags, Length: 5012, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, line in enumerate(data_set_frame[0]):\n",
    "    sent = nltk.tokenize.WordPunctTokenizer().tokenize(line)\n",
    "    tag_line = []\n",
    "    for tag in tagger.tag(sent):\n",
    "        tag_line.append(tag[1])\n",
    "    data_set_frame.at[i, 'PosTags'] = tag_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Classification\\\\nltk_german_classifier_data.pickle', 'rb') as f:\n",
    "    tagger = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bag = set(tag for index, row in data_set_frame.iterrows() for tag in row['PosTags'])\n",
    "feature_set_pos= [([(tag in row['PosTags']) for tag in tag_bag], row[1]) for item, row in data_set_frame.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_frame_pos = pd.DataFrame(feature_set_pos)\n",
    "x_pos, y_pos = bag_frame_pos.iloc[:,:-1],bag_frame_pos.iloc[:,-1]\n",
    "x_frame_pos = pd.DataFrame(x_pos[0].tolist(), columns = tag_bag)\n",
    "x_pos_train, x_pos_test, y_pos_train, y_pos_test = train_test_split(x_frame_pos, y_pos, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953140578265204"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_class_pos = xgb.XGBClassifier(max_depth=6, n_estimators=125, learning_rate=0.125, min_child_weight = 1, njobs=4)\n",
    "xg_class_pos.fit(x_pos_train, y_pos_train)\n",
    "preds = xg_class_pos.predict(x_pos_test)\n",
    "accuracy_score(preds, y_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.90895, std: 0.01659, params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100},\n",
       "  mean: 0.90920, std: 0.01504, params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 125},\n",
       "  mean: 0.90970, std: 0.01514, params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150},\n",
       "  mean: 0.91094, std: 0.01250, params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100},\n",
       "  mean: 0.91020, std: 0.01331, params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 125},\n",
       "  mean: 0.91020, std: 0.01231, params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 150},\n",
       "  mean: 0.90920, std: 0.01129, params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100},\n",
       "  mean: 0.90870, std: 0.01060, params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 125},\n",
       "  mean: 0.90695, std: 0.01219, params: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 150},\n",
       "  mean: 0.91194, std: 0.01447, params: {'learning_rate': 0.125, 'max_depth': 5, 'n_estimators': 100},\n",
       "  mean: 0.91094, std: 0.01418, params: {'learning_rate': 0.125, 'max_depth': 5, 'n_estimators': 125},\n",
       "  mean: 0.91095, std: 0.01510, params: {'learning_rate': 0.125, 'max_depth': 5, 'n_estimators': 150},\n",
       "  mean: 0.91145, std: 0.01299, params: {'learning_rate': 0.125, 'max_depth': 6, 'n_estimators': 100},\n",
       "  mean: 0.91244, std: 0.01166, params: {'learning_rate': 0.125, 'max_depth': 6, 'n_estimators': 125},\n",
       "  mean: 0.91070, std: 0.00967, params: {'learning_rate': 0.125, 'max_depth': 6, 'n_estimators': 150},\n",
       "  mean: 0.90745, std: 0.01079, params: {'learning_rate': 0.125, 'max_depth': 7, 'n_estimators': 100},\n",
       "  mean: 0.90895, std: 0.00944, params: {'learning_rate': 0.125, 'max_depth': 7, 'n_estimators': 125},\n",
       "  mean: 0.91020, std: 0.00849, params: {'learning_rate': 0.125, 'max_depth': 7, 'n_estimators': 150},\n",
       "  mean: 0.89399, std: 0.01446, params: {'learning_rate': 1.5, 'max_depth': 5, 'n_estimators': 100},\n",
       "  mean: 0.89324, std: 0.01457, params: {'learning_rate': 1.5, 'max_depth': 5, 'n_estimators': 125},\n",
       "  mean: 0.89050, std: 0.01399, params: {'learning_rate': 1.5, 'max_depth': 5, 'n_estimators': 150},\n",
       "  mean: 0.89449, std: 0.01871, params: {'learning_rate': 1.5, 'max_depth': 6, 'n_estimators': 100},\n",
       "  mean: 0.89299, std: 0.01754, params: {'learning_rate': 1.5, 'max_depth': 6, 'n_estimators': 125},\n",
       "  mean: 0.89125, std: 0.01757, params: {'learning_rate': 1.5, 'max_depth': 6, 'n_estimators': 150},\n",
       "  mean: 0.89125, std: 0.01440, params: {'learning_rate': 1.5, 'max_depth': 7, 'n_estimators': 100},\n",
       "  mean: 0.89000, std: 0.01592, params: {'learning_rate': 1.5, 'max_depth': 7, 'n_estimators': 125},\n",
       "  mean: 0.88875, std: 0.01565, params: {'learning_rate': 1.5, 'max_depth': 7, 'n_estimators': 150}],\n",
       " {'learning_rate': 0.125, 'max_depth': 6, 'n_estimators': 125},\n",
       " 0.9124438733313653)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 =  {\n",
    " 'learning_rate':[0.1,0.125, 1.5],\n",
    " 'n_estimators':[100, 125, 150],\n",
    " 'max_depth': [5, 6, 7]}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate = 0.1,\n",
    "                                                      max_depth = 5,\n",
    "                                                      random_state= 10,\n",
    "                                                      min_child_weight = 1), \n",
    "param_grid = param_test1, n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.estimator.get_params()\n",
    "gsearch1.fit(np.array(x_pos_train), np.array(y_pos_train))\n",
    "\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ngrams_bag = set(gram for index, row in data_set_frame.iterrows() for gram in ngrams(row['PosTags'], 2))\n",
    "feature_set_pos_ngrams = [([(gram in ngrams(row['PosTags'], 2)) for tag in tag_ngrams_bag], row[1])for item, row in data_set_frame.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
